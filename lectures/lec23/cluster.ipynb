{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# CS 429: Information Retrieval\n",
      "<br>\n",
      "\n",
      "## Lecture 23: Clustering Words\n",
      "\n",
      "<br>\n",
      "\n",
      "### Dr. Aron Culotta\n",
      "### Illinois Institute of Technology\n",
      "### Spring 2014"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Motivation\n",
      "\n",
      "Often, we want to know which features appear together.\n",
      "\n",
      "- If you liked *Twilight* you might like *Nosferatu*.\n",
      "- \"happy\" is a synonym of \"glad.\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll use k-means to cluster together related words from Twitter.\n",
      "\n",
      "**Caution:** This uses live Twitter data, which often contains profanity."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get some tweets containing the word 'i'.\n",
      "\n",
      "import os\n",
      "from TwitterAPI import TwitterAPI\n",
      "\n",
      "# Read Twitter credentials from environmental variables.\n",
      "api = TwitterAPI(os.environ.get('TW_CONSUMER_KEY'),\n",
      "                 os.environ.get('TW_CONSUMER_SECRET'),\n",
      "                 os.environ.get('TW_ACCESS_TOKEN'),\n",
      "                 os.environ.get('TW_ACCESS_TOKEN_SECRET'))\n",
      "\n",
      "# Collect tweets until we hit rate limit.\n",
      "tweets = []\n",
      "while True: \n",
      "    r = api.request('search/tweets', {'q':'i',\n",
      "                                      'language':'en',\n",
      "                                      'count':'100'})\n",
      "    if r.status_code != 200: # error\n",
      "        break\n",
      "    else:\n",
      "        for item in r.get_iterator():\n",
      "            tweets.append(item)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "17300\n"
       ]
      }
     ],
     "prompt_number": 416
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Each tweet is a Python dict.\n",
      "print 'text', tweets[10]['text']\n",
      "print 'description:', tweets[10]['user']['description']\n",
      "print 'name:', tweets[10]['user']['name']\n",
      "print 'location:', tweets[10]['user']['location']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "text \"You are amazing.\" - Silas says to Sam. I have to agree. #GH\n",
        "description: I am a former editor.  A soon to be grad student and educator.  I currently have a book about my life in editing mode.\n",
        "name: Sean Greeley\n",
        "location: Oakland Gardens, New York\n"
       ]
      }
     ],
     "prompt_number": 417
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tokenize each tweet text.\n",
      "import re\n",
      "tokens = []\n",
      "for tweet in tweets:\n",
      "    text = tweet['text'].lower()\n",
      "    text = re.sub('@\\S+', ' ', text)  # Remove mentions.\n",
      "    text = re.sub('http\\S+', ' ', text)  # Remove urls.\n",
      "    tokens.append(re.findall('[A-Za-z]+', text)) # Retain words.\n",
      "print tokens[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'i', u'm', u'strangely', u'ok', u'with', u'being', u'known', u'as', u'a', u'day', u'drinker']\n"
       ]
      }
     ],
     "prompt_number": 418
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Count words.\n",
      "from collections import Counter\n",
      "\n",
      "word_counts = Counter()\n",
      "for tweet in tokens:\n",
      "    word_counts.update(tweet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 419
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Inspect word counts.\n",
      "import math\n",
      "\n",
      "print len(word_counts), 'unique terms'\n",
      "sorted_counts = sorted(word_counts.items(),\n",
      "                       key=lambda x: x[1],\n",
      "                       reverse=True)\n",
      "print '\\n'.join('%s\\t%d' % (w, c) for w, c in sorted_counts[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "18713 unique terms\n",
        "i\t21239\n",
        "rt\t5812\n",
        "to\t5137\n",
        "you\t4381\n",
        "the\t4192\n",
        "a\t4121\n",
        "m\t3303\n",
        "my\t3070\n",
        "and\t2891\n",
        "me\t2768\n"
       ]
      }
     ],
     "prompt_number": 420
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Retain in vocabulary words occurring more than twice.\n",
      "vocab = set([w for w, c in word_counts.iteritems() if c > 2])\n",
      "print '%d words occur at least three times.' % len(vocab)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4915 words occur at least three times.\n"
       ]
      }
     ],
     "prompt_number": 421
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Prune tokens.\n",
      "newtoks = []\n",
      "for i, tweet in enumerate(tokens):\n",
      "    newtok = [token for token in tweet if token in vocab]\n",
      "    if len(newtok) > 0:\n",
      "        newtoks.append(newtok)\n",
      "tokens = newtoks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 422
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A sample pruned tweet.\n",
      "print tokens[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'i', u'm', u'ok', u'with', u'being', u'known', u'as', u'a', u'day']\n"
       ]
      }
     ],
     "prompt_number": 423
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# For each term, create a context vector, indicating how often\n",
      "# each word occurs to the left or right of it.\n",
      "from collections import defaultdict\n",
      "\n",
      "# dict from term to context vector.\n",
      "contexts = defaultdict(lambda: Counter())\n",
      "window = 2\n",
      "for tweet in tokens:\n",
      "    for i, token in enumerate(tweet):\n",
      "        features = []\n",
      "        for j in range(np.amax([0, i-window]), i):\n",
      "            features.append(tweet[j] + \"@\" + str(j-i))\n",
      "        for j in range(i+1, min(i + window, len(tweet))):\n",
      "            features.append(tweet[j] + \"@\" + str(j-i))\n",
      "        contexts[token].update(features)\n",
      "        # Optionally: ignore word order\n",
      "        # contexts[token].update(tweet[:i] + tweet[i+1:])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 426
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print contexts['you'].items()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'talk@-2', 13), (u'touch@-2', 1), (u'please@1', 32), (u'man@-1', 1), (u'date@-1', 2)]\n"
       ]
      }
     ],
     "prompt_number": 428
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute the number of different contexts each term appears in.\n",
      "tweet_freq = Counter()\n",
      "for context in contexts.itervalues():\n",
      "    for term in context:\n",
      "        tweet_freq[term] += 1.\n",
      "print tweet_freq.items()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'please@1', 90.0), (u'history@-1', 7.0), (u'sum@1', 11.0), (u'date@-2', 22.0), (u'history@-2', 8.0)]\n"
       ]
      }
     ],
     "prompt_number": 429
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Transform each context vector to be term freq / tweet frequency. \n",
      "# Also then normalize by length.\n",
      "for term, context in contexts.iteritems():\n",
      "    for term2, frequency in context.iteritems():\n",
      "        context[term2] = frequency / (1. + math.log(tweet_freq[term2]))\n",
      "    length = math.sqrt(sum([v*v for v in context.itervalues()]))\n",
      "    for term2, frequency in context.iteritems():\n",
      "        context[term2] = 1. * frequency / length\n",
      "    \n",
      "print contexts['being'].items()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'comes@-2', 0.021457572258183157), (u'or@1', 0.013519637819578057), (u'as@-1', 0.030798751481017773), (u'time@-1', 0.030514456587444547), (u'wow@1', 0.02101070082357469)]\n"
       ]
      }
     ],
     "prompt_number": 430
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point we have ~5k dictionaries, one per term, indicating the terms that co-occur (weighted by inverse tweet frequency).\n",
      "\n",
      "Next, we have to cluster these vectors. To do this, we'll need to be able to compute the euclidean distance between two vectors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def distance(c1, c2):\n",
      "    if len(c1.keys()) == 0 or len(c2.keys()) == 0:\n",
      "        return 1e9\n",
      "    keys = set(c1.keys()) | set(c2.keys())\n",
      "    distance = 0.\n",
      "    for k in keys:\n",
      "        distance += (c1[k] - c2[k]) ** 2\n",
      "    return math.sqrt(distance)\n",
      "\n",
      "print distance({'hi':10, 'bye': 5}, {'hi': 9, 'bye': 4})\n",
      "print distance({'hi':10, 'bye': 5}, {'hi': 8, 'bye': 4})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.41421356237\n",
        "2.2360679775\n"
       ]
      }
     ],
     "prompt_number": 431
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_closest(term, n=5):\n",
      "    terms = np.array(contexts.keys())\n",
      "    context = contexts[term]\n",
      "    distances = []\n",
      "    for term2, context2 in contexts.iteritems():\n",
      "        distances.append(distance(context, context2))\n",
      "    return terms[np.argsort(distances)][:n]\n",
      "\n",
      "print find_closest('sad', n=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'sad' u'sleepy' u'excited' u'glad' u'sorry' u'scared' u'lazy' u'bad'\n",
        " u'hungry' u'nervous']\n"
       ]
      }
     ],
     "prompt_number": 432
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nz_contexts = [t for t, context in contexts.items()\n",
      "               if len(context) > 1]\n",
      "contexts = dict([(term, contexts[term]) for term in nz_contexts])\n",
      "print len(nz_contexts), 'nonzero contexts'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4910 nonzero contexts\n"
       ]
      }
     ],
     "prompt_number": 436
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Transform context dicts to a sparse vector\n",
      "# for sklearn.\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "\n",
      "vec = DictVectorizer()\n",
      "X = vec.fit_transform(contexts.values())\n",
      "names = np.array(vec.get_feature_names())\n",
      "print names[:10]\n",
      "print X[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'a@-1' u'a@-2' u'a@1' u'aap@-1' u'aap@-2' u'aap@1' u'aapl@-1' u'aapl@-2'\n",
        " u'aapl@1' u'aaron@-1']\n",
        "  (0, 4941)\t0.237473316099\n",
        "  (0, 7742)\t0.374375711285\n",
        "  (0, 6262)\t0.146969238152\n",
        "  (0, 2252)\t0.374375711285\n",
        "  (0, 13400)\t0.166342996686\n",
        "  (0, 10740)\t0.143485404568\n",
        "  (0, 8758)\t0.163703930656\n",
        "  (0, 7285)\t0.339401454089\n",
        "  (0, 2191)\t0.229437338908\n",
        "  (0, 8608)\t0.61729242976\n",
        "  (0, 1)\t0.133230276709\n"
       ]
      }
     ],
     "prompt_number": 437
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's cluster!\n",
      "from sklearn.cluster import KMeans\n",
      "num_clusters = 50\n",
      "kmeans = KMeans(num_clusters)\n",
      "kmeans.fit(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 438,
       "text": [
        "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=50, n_init=10,\n",
        "    n_jobs=1, precompute_distances=True, random_state=None, tol=0.0001,\n",
        "    verbose=0)"
       ]
      }
     ],
     "prompt_number": 438
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's print out the top features for each mean vector.\n",
      "# This is swamped by common terms\n",
      "for i in range(num_clusters):\n",
      "    print i, ' '.join(names[np.argsort(kmeans.cluster_centers_[i])[::-1][:5]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 i@-2 i@-1 just@-1 ve@-1 a@1\n",
        "1 the@-1 i@1 in@-2 to@-2 and@1\n",
        "2 like@1 it@-1 elephant@-2 to@-1 starting@-2\n",
        "3 be@-1 to@-2 i@-2 t@-2 being@-1\n",
        "4 a@-1 i@1 my@-1 the@-1 a@-2\n",
        "5 you@1 i@-2 i@-1 i@1 me@1\n",
        "6 t@-1 can@-2 don@-2 i@-1 to@-1\n",
        "7 i@-2 m@-1 m@-2 i@1 be@-1\n",
        "8 my@-2 my@-1 i@1 the@-2 a@-1\n",
        "9 an@-1 i@1 with@1 a@-2 for@-2\n",
        "10 she@-1 it@-1 her@-2 he@-1 door@-2\n",
        "11 your@-1 my@-1 in@-2 the@-1 i@1\n",
        "12 up@1 i@-2 i@-1 a@-1 to@-1\n",
        "13 the@-1 the@-2 in@-2 i@1 of@-2\n",
        "14 and@-1 is@-1 he@-1 for@-1 the@1\n",
        "15 a@-2 a@-1 i@1 but@1 and@1\n",
        "16 i@-2 love@-1 m@-1 i@1 i@-1\n",
        "17 proud@-2 to@-1 pleased@-2 i@1 rider@1\n",
        "18 with@1 i@-2 to@1 m@-2 m@-1\n",
        "19 in@-1 i@1 to@-1 for@-1 and@1\n",
        "20 me@1 i@-1 you@1 you@-1 to@-1\n",
        "21 to@-1 the@1 i@1 i@-1 to@1\n",
        "22 lil@-1 gucci@-1 nia@-2 lady@1 essential@-1\n",
        "23 i@-1 i@-2 rt@-2 i@1 a@1\n",
        "24 i@-1 i@-2 a@1 rt@-2 i@1\n",
        "25 of@1 the@-1 a@-1 i@-2 the@-2\n",
        "26 san@-1 santa@-1 at@-2 churrascaria@-1 his@-1\n",
        "27 de@1 de@-1 de@-2 m@-2 at@-1\n",
        "28 to@-2 i@1 see@-1 the@-1 and@1\n",
        "29 i@1 rt@-1 i@-2 the@-2 you@1\n",
        "30 and@1 i@-2 the@-1 the@-2 my@-1\n",
        "31 my@-1 i@1 in@-2 and@1 on@-2\n",
        "32 i@-2 to@-2 i@1 the@1 for@-1\n",
        "33 of@-1 the@-1 i@1 out@-2 i@-2\n",
        "34 fa@1 oy@-2 santiago@-2 cia@-1 tylko@-1\n",
        "35 my@1 the@1 i@-2 i@-1 a@1\n",
        "36 so@-1 m@-2 i@-2 i@1 m@-1\n",
        "37 gain@-2 gain@-1 rts@-2 followtrick@-1 teamfollowback@-1\n",
        "38 from@-1 video@-2 national@-1 day@1 i@1\n",
        "39 on@-1 i@1 on@-2 the@-1 the@-2\n",
        "40 in@1 i@-2 the@-2 get@-1 to@-2\n",
        "41 i@1 rt@-1 the@-1 i@-2 a@-2\n",
        "42 to@1 i@-2 i@-1 m@-1 not@-1\n",
        "43 rt@-1 rt@-2 i@-1 the@-2 i@-2\n",
        "44 a@-1 i@1 and@1 the@-1 of@1\n",
        "45 to@-1 i@-2 i@-1 it@1 my@1\n",
        "46 you@-1 i@1 love@-2 i@-2 you@-2\n",
        "47 t@1 i@-1 you@-1 if@-2 it@-1\n",
        "48 flaherty@1 partnered@-2 rip@-1 pag@-1 affected@-2\n",
        "49 on@1 i@-2 the@-1 a@-1 a@-2\n"
       ]
      }
     ],
     "prompt_number": 439
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# .transform will compute the distance from each term to each cluster.\n",
      "distances = kmeans.transform(X)\n",
      "print distances[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.13521267  1.1240182   1.10742592  1.08732169  1.03694299  1.04006013\n",
        "  1.15302655  1.15812103  1.03107212  1.08102953  1.10724202  1.05408489\n",
        "  1.11956906  1.02070019  1.0508641   1.01893944  1.0311229   1.1370101\n",
        "  1.05999362  1.010669    1.0954494   1.07168689  1.02534256  1.22808261\n",
        "  1.05724649  1.10921475  1.08896775  1.0245512   1.03153311  1.12516247\n",
        "  1.02367389  1.10663023  1.09598667  1.05891253  1.04467328  1.07224092\n",
        "  1.12788733  1.09460034  1.0410211   1.06544318  1.06445839  1.0165742\n",
        "  1.14504997  0.99765603  1.12288503  1.13720466  1.04061974  1.37525835\n",
        "  1.07813631  1.06099406]\n"
       ]
      }
     ],
     "prompt_number": 440
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Finally, we'll print the words that are closest\n",
      "# to the mean of each cluster.\n",
      "terms = np.array(contexts.keys())\n",
      "for i in range(num_clusters):\n",
      "    print i, ' '.join(terms[np.argsort(distances[:,i])[1:10]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 saw got entered checked realized learned heard had made\n",
        "1 world truth worst gym bathroom devil beach masters most\n",
        "2 looks mctc feel titties feels sleeps essays look fly\n",
        "3 bothered loyal honest expressed surprised attending appreciated able trusted\n",
        "4 few little joke couple guy job good stoner boyfriend\n",
        "5 cursing ship meet if invite miss annoying stalk tell\n",
        "6 wait even stand breathe contain stop handle blame care\n",
        "7 hungry guessing melting confused gonna lien getting afraid buried\n",
        "8 yard dream glasses nationalsiblingday teacher liam twin watches brother\n",
        "9 hour option adult indirect egg assignment argument upgrade acc\n",
        "10 stays raps bent moaned screams tasted s takes hurts\n",
        "11 face opinion butt stomach life mouth head heart car\n",
        "12 wake fucked hurry messed ended give woken waking catching\n",
        "13 same beach bathroom worst truth most masters way gym\n",
        "14 hayley jazmyn adjust shake follows adj chase then signing\n",
        "15 mood favor person photo kid pace tool tan buddy\n",
        "16 you getting u probably afraid hungry done some a\n",
        "17 satisfied lasted host boli say work share do play\n",
        "18 partnered content quote done deeply pleased deal agree concerned\n",
        "19 class japan town front depth politics ohio bed cleveland\n",
        "20 excuse tell remind gave make reminds ignore inspired followed\n",
        "21 play eat draw clarify go brush meet sleep spend\n",
        "22 sis punk rob freaky mane doe steve wayne czym\n",
        "23 hate got swear am hope volunteer ll think thought\n",
        "24 got hate am hope swear have volunteer thought just\n",
        "25 amount type sort instead none part rest lots pair\n",
        "26 fransico monica cruz pride rn sons spf ce maria\n",
        "27 deus de banda el centro las gol e hospital\n",
        "28 ourselves playlist bathroom dms moon portland pausing him them\n",
        "29 but ahhh cause yes yep yeah lol and because\n",
        "30 paradise fun prison pencil legend gps brothers yogurt favour\n",
        "31 tl phone wrist hair heart dad car soul own\n",
        "32 u a him not that some getting them afraid\n",
        "33 justice habit india couples college weirdos those duty nowhere\n",
        "34 madre vuruyor nun qu oy sms ada chcecie metropolitana\n",
        "35 stole updated wasting in rearranged change packing washed into\n",
        "36 much sleepy cute hard glad lazy bad far upset\n",
        "37 followtrick onedirection ua tcfollowtrain mm teamfollowback ipadgames retweet vod\n",
        "38 dallas minecraft argentina sos congress throwbackthursday battle mw monsters\n",
        "39 soundcloud yelp youtube sunday speak tumblr saturday holiday instagram\n",
        "40 faith performing stuck checked participate interested masturbate landed involved\n",
        "41 when ahhh cause yes and lol yep yeah because\n",
        "42 trying listen unable listening used addicted going talk able\n",
        "43 do one all lol like shit man and for\n",
        "44 few couple little joke stoner cookie cassette nap chance\n",
        "45 find watch take be do say make put play\n",
        "46 were guys shawty are spinnrtaylorswift inspired happybdayshaymitchellfrombrazil bby join\n",
        "47 ain wouldn didn haven couldn wasn won shouldn hadn\n",
        "48 mr cdnpoli parallels makism autism gising tayo society mplaces\n",
        "49 focus focusing hooked depends working lock focused poop banged\n"
       ]
      }
     ],
     "prompt_number": 441
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clearly, interpreting these results requires a bit of investigation.\n",
      "\n",
      "Some patterns do emerge:\n",
      " - Cluster 47 is all contractions\n",
      " - Cluster 29,41 seems to be inerjections/[disfluencies](http://en.wikipedia.org/wiki/Speech_disfluency)\n",
      " - Cluster 39 refers to tech sites.\n",
      " - Cluster 34 has Polish and what looks like Spanish.\n",
      "\n",
      "As the number of tweets increases, we expect these clusters to become more coherent."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}