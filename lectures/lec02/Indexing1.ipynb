{
 "metadata": {
  "name": "",
  "signature": "sha256:c6773400e06123353e55cd3a192f7824d15a62dd35c4230e0237efd8f38ab08f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# CS 429: Information Retrieval\n",
      "\n",
      "<br>\n",
      "## Lecture 2: Indexing\n",
      "\n",
      "<br>\n",
      "\n",
      "### Dr. Aron Culotta\n",
      "### Illinois Institute of Technology \n",
      "### Spring 2015\n",
      "\n",
      "---\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Indexing Pipeline\n",
      "\n",
      "1. Collect documents\n",
      "2. Tokenize\n",
      "3. Normalize\n",
      "4. Index"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Indexing Pipeline\n",
      "\n",
      "document $\\xrightarrow{tokenize}$ (tokens, types) $\\xrightarrow{normalize}$ terms $\\xrightarrow{index}$ inverted index"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# 1. Collect documents.\n",
      "document = \"he didn't know where he worked.\"\n",
      "\n",
      "# 2. Tokenize\n",
      "tokens = [\"he\", \"didn't\", \"know\", \"where\", \"he\", \"worked\"] # split; remove punctuation\n",
      "types = [\"he\", \"didn't\", \"know\", \"where\", \"worked\"] # unique tokens.\n",
      "\n",
      "# 3. Normalize\n",
      "# remove common words like 'where'; collapse word forms like worked -> work\n",
      "terms = [\"he\", \"didnt\", \"know\", \"work\"]\n",
      "\n",
      "# 4. Index\n",
      "index = {'he': [0],\n",
      "         'didnt': [0],\n",
      "         'know' : [0],\n",
      "         'work' : [0]}"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "**e\u00b7quiv\u00b7a\u00b7lence class** (/i'kwiv\u0259l\u0259ns klas/) *n.*\n",
      "\n",
      "1. A subset whose elements are equivalent according to some relation $\\sim$ .\n",
      "\n",
      "    $S' \\subseteq S \\hspace{.2cm}$ s.t. $\\hspace{.2cm}x_i \\sim x_j \\hspace{.1cm} \\forall x_i, x_j \\in S'$\n",
      "    \n",
      "    E.g., consider the set \\{dog, cat, spider\\} and the relation \"has same number of legs\". Then there are two equivalence classes: \\{dog, cat\\} and \\{spider\\}."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "**to\u00b7ken** (/t\u014dk\u0259n/) *n*.\n",
      "\n",
      "1. A sequence of characters in a document that form a meaningful unit.\n",
      "2. The output of a *tokenizer*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**type** (/t\u012bp/) *n.*\n",
      "\n",
      "1. An equivalence class of *tokens* under the string equality relation.\n",
      "2. By analogy to OO-programming, class:object :: type:token\n",
      "\n",
      "    *e.g., \"to be or not to be\"* $\\xrightarrow{tokenize}$ *tokens={to, be, or, not, to, be}* $\\hspace{.2cm}$  *types={to, be, or, not}*\n",
      "    \n",
      "    The type 'to' is an equivalence class containing the first and fifth tokens of the document.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "**term** (/t\u0259rm/) *n.*\n",
      "\n",
      "1. An equivalence class of *types* under the relation \"have the same normalized form.\"\n",
      "2. The keys in the inverted index.\n",
      "\n",
      "    e.g., types={John, john, aren't, arent} $\\xrightarrow{normalize}$ terms={john, arent}.\n",
      "\n",
      "    The term \"john\" is an equivalence class containing types {John, john}.\n",
      "    \n",
      "    The term \"arent\" is an equivalence class containing types {arent, aren't}.\n",
      "    \n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Tokenization\n",
      "\n",
      "**to\u00b7ken\u00b7i\u00b7za\u00b7tion** (/\u02c8t\u014dk\u0259n iz\u0101-sh\u0259n/) *n.*\n",
      "\n",
      "1. The process of splitting a document into tokens.\n",
      "\n",
      "    *Simplest approach*: split on whitespace.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print document.split()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['he', \"didn't\", 'know', 'where', 'he', 'worked.']\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'what     about multiple     spaces?'.split()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['what', 'about', 'multiple', 'spaces?']\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Tokenization: Compound Nouns\n",
      "\n",
      "- *San Francisco*;  *New York University* vs *York University*\n",
      "- Solved somewhat by *phrase indexing* (next class)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Tokenization: Segmentation\n",
      "\n",
      "- *Lebensversicherungsgesellschaftsangestellter*\n",
      "  - \"life insurance company employee\"\n",
      "- \u6211\u4e0d\u80fd\u8bf4\u4e2d\u56fd\u8bdd\n",
      "- *\\#androidgames*\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Statistical classification algorithms can be used to split (Part III of course). \n",
      "- Simpler: index character subsequences (*n-grams*).\n",
      "  - E.g., *\\#androidgames* $\\rightarrow$ {*#andr, andro, ndroi, droid, roidg, ..., games*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Tokenization: Punctuation\n",
      "\n",
      "- Remove all punctuation? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "  - \"didn't\", \"www.google.com\"\n",
      "  - [CAR](https://www.google.com/search?q=CAR) vs [C.A.R](https://www.google.com/search?q=C.A.R.).\n",
      "  - [O'Neill vs ONeill vs O Neill](https://www.google.com/#q=oneill+-o'neill&safe=active)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Tokenization: Regular Expressions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "symbol | meaning\n",
      "------ | -------\n",
      "\\b\t| Word boundary (zero width)\n",
      "\\d\t| Any decimal digit (equivalent to [0-9])\n",
      "\\D\t| Any non-digit character (equivalent to [^0-9])\n",
      "\\s\t| Any whitespace character (equivalent to [ \\t\\n\\r\\f\\v]\n",
      "\\S\t| Any non-whitespace character (equivalent to [^ \\t\\n\\r\\f\\v])\n",
      "\\w\t| Any alphanumeric character (equivalent to [a-zA-Z0-9_])\n",
      "\\W\t| Any non-alphanumeric character (equivalent to [^a-zA-Z0-9_])\n",
      "\\t\t| The tab character\n",
      "\\n\t| The newline character\n",
      "\n",
      "(source: <http://nltk.org/book/ch03.html>)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import re  # Regular expression module\n",
      "print re.split('x', 'axbxc')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['a', 'b', 'c']\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "help(re.split)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on function split in module re:\n",
        "\n",
        "split(pattern, string, maxsplit=0, flags=0)\n",
        "    Split the source string by the occurrences of the pattern,\n",
        "    returning a list containing the resulting substrings.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "help(re)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on module re:\n",
        "\n",
        "NAME\n",
        "    re - Support for regular expressions (RE).\n",
        "\n",
        "FILE\n",
        "    /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/re.py\n",
        "\n",
        "MODULE DOCS\n",
        "    http://docs.python.org/library/re\n",
        "\n",
        "DESCRIPTION\n",
        "    This module provides regular expression matching operations similar to\n",
        "    those found in Perl.  It supports both 8-bit and Unicode strings; both\n",
        "    the pattern and the strings being processed can contain null bytes and\n",
        "    characters outside the US ASCII range.\n",
        "    \n",
        "    Regular expressions can contain both special and ordinary characters.\n",
        "    Most ordinary characters, like \"A\", \"a\", or \"0\", are the simplest\n",
        "    regular expressions; they simply match themselves.  You can\n",
        "    concatenate ordinary characters, so last matches the string 'last'.\n",
        "    \n",
        "    The special characters are:\n",
        "        \".\"      Matches any character except a newline.\n",
        "        \"^\"      Matches the start of the string.\n",
        "        \"$\"      Matches the end of the string or just before the newline at\n",
        "                 the end of the string.\n",
        "        \"*\"      Matches 0 or more (greedy) repetitions of the preceding RE.\n",
        "                 Greedy means that it will match as many repetitions as possible.\n",
        "        \"+\"      Matches 1 or more (greedy) repetitions of the preceding RE.\n",
        "        \"?\"      Matches 0 or 1 (greedy) of the preceding RE.\n",
        "        *?,+?,?? Non-greedy versions of the previous three special characters.\n",
        "        {m,n}    Matches from m to n repetitions of the preceding RE.\n",
        "        {m,n}?   Non-greedy version of the above.\n",
        "        \"\\\\\"     Either escapes special characters or signals a special sequence.\n",
        "        []       Indicates a set of characters.\n",
        "                 A \"^\" as the first character indicates a complementing set.\n",
        "        \"|\"      A|B, creates an RE that will match either A or B.\n",
        "        (...)    Matches the RE inside the parentheses.\n",
        "                 The contents can be retrieved or matched later in the string.\n",
        "        (?iLmsux) Set the I, L, M, S, U, or X flag for the RE (see below).\n",
        "        (?:...)  Non-grouping version of regular parentheses.\n",
        "        (?P<name>...) The substring matched by the group is accessible by name.\n",
        "        (?P=name)     Matches the text matched earlier by the group named name.\n",
        "        (?#...)  A comment; ignored.\n",
        "        (?=...)  Matches if ... matches next, but doesn't consume the string.\n",
        "        (?!...)  Matches if ... doesn't match next.\n",
        "        (?<=...) Matches if preceded by ... (must be fixed length).\n",
        "        (?<!...) Matches if not preceded by ... (must be fixed length).\n",
        "        (?(id/name)yes|no) Matches yes pattern if the group with id/name matched,\n",
        "                           the (optional) no pattern otherwise.\n",
        "    \n",
        "    The special sequences consist of \"\\\\\" and a character from the list\n",
        "    below.  If the ordinary character is not on the list, then the\n",
        "    resulting RE will match the second character.\n",
        "        \\number  Matches the contents of the group of the same number.\n",
        "        \\A       Matches only at the start of the string.\n",
        "        \\Z       Matches only at the end of the string.\n",
        "        \\b       Matches the empty string, but only at the start or end of a word.\n",
        "        \\B       Matches the empty string, but not at the start or end of a word.\n",
        "        \\d       Matches any decimal digit; equivalent to the set [0-9].\n",
        "        \\D       Matches any non-digit character; equivalent to the set [^0-9].\n",
        "        \\s       Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v].\n",
        "        \\S       Matches any non-whitespace character; equiv. to [^ \\t\\n\\r\\f\\v].\n",
        "        \\w       Matches any alphanumeric character; equivalent to [a-zA-Z0-9_].\n",
        "                 With LOCALE, it will match the set [0-9_] plus characters defined\n",
        "                 as letters for the current locale.\n",
        "        \\W       Matches the complement of \\w.\n",
        "        \\\\       Matches a literal backslash.\n",
        "    \n",
        "    This module exports the following functions:\n",
        "        match    Match a regular expression pattern to the beginning of a string.\n",
        "        search   Search a string for the presence of a pattern.\n",
        "        sub      Substitute occurrences of a pattern found in a string.\n",
        "        subn     Same as sub, but also return the number of substitutions made.\n",
        "        split    Split a string by the occurrences of a pattern.\n",
        "        findall  Find all occurrences of a pattern in a string.\n",
        "        finditer Return an iterator yielding a match object for each match.\n",
        "        compile  Compile a pattern into a RegexObject.\n",
        "        purge    Clear the regular expression cache.\n",
        "        escape   Backslash all non-alphanumerics in a string.\n",
        "    \n",
        "    Some of the functions in this module takes flags as optional parameters:\n",
        "        I  IGNORECASE  Perform case-insensitive matching.\n",
        "        L  LOCALE      Make \\w, \\W, \\b, \\B, dependent on the current locale.\n",
        "        M  MULTILINE   \"^\" matches the beginning of lines (after a newline)\n",
        "                       as well as the string.\n",
        "                       \"$\" matches the end of lines (before a newline) as well\n",
        "                       as the end of the string.\n",
        "        S  DOTALL      \".\" matches any character at all, including the newline.\n",
        "        X  VERBOSE     Ignore whitespace and comments for nicer looking RE's.\n",
        "        U  UNICODE     Make \\w, \\W, \\b, \\B, dependent on the Unicode locale.\n",
        "    \n",
        "    This module also defines an exception 'error'.\n",
        "\n",
        "CLASSES\n",
        "    exceptions.Exception(exceptions.BaseException)\n",
        "        sre_constants.error\n",
        "    \n",
        "    class error(exceptions.Exception)\n",
        "     |  Method resolution order:\n",
        "     |      error\n",
        "     |      exceptions.Exception\n",
        "     |      exceptions.BaseException\n",
        "     |      __builtin__.object\n",
        "     |  \n",
        "     |  Data descriptors defined here:\n",
        "     |  \n",
        "     |  __weakref__\n",
        "     |      list of weak references to the object (if defined)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from exceptions.Exception:\n",
        "     |  \n",
        "     |  __init__(...)\n",
        "     |      x.__init__(...) initializes x; see help(type(x)) for signature\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data and other attributes inherited from exceptions.Exception:\n",
        "     |  \n",
        "     |  __new__ = <built-in method __new__ of type object>\n",
        "     |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Methods inherited from exceptions.BaseException:\n",
        "     |  \n",
        "     |  __delattr__(...)\n",
        "     |      x.__delattr__('name') <==> del x.name\n",
        "     |  \n",
        "     |  __getattribute__(...)\n",
        "     |      x.__getattribute__('name') <==> x.name\n",
        "     |  \n",
        "     |  __getitem__(...)\n",
        "     |      x.__getitem__(y) <==> x[y]\n",
        "     |  \n",
        "     |  __getslice__(...)\n",
        "     |      x.__getslice__(i, j) <==> x[i:j]\n",
        "     |      \n",
        "     |      Use of negative indices is not supported.\n",
        "     |  \n",
        "     |  __reduce__(...)\n",
        "     |  \n",
        "     |  __repr__(...)\n",
        "     |      x.__repr__() <==> repr(x)\n",
        "     |  \n",
        "     |  __setattr__(...)\n",
        "     |      x.__setattr__('name', value) <==> x.name = value\n",
        "     |  \n",
        "     |  __setstate__(...)\n",
        "     |  \n",
        "     |  __str__(...)\n",
        "     |      x.__str__() <==> str(x)\n",
        "     |  \n",
        "     |  __unicode__(...)\n",
        "     |  \n",
        "     |  ----------------------------------------------------------------------\n",
        "     |  Data descriptors inherited from exceptions.BaseException:\n",
        "     |  \n",
        "     |  __dict__\n",
        "     |  \n",
        "     |  args\n",
        "     |  \n",
        "     |  message\n",
        "\n",
        "FUNCTIONS\n",
        "    compile(pattern, flags=0)\n",
        "        Compile a regular expression pattern, returning a pattern object.\n",
        "    \n",
        "    escape(pattern)\n",
        "        Escape all non-alphanumeric characters in pattern.\n",
        "    \n",
        "    findall(pattern, string, flags=0)\n",
        "        Return a list of all non-overlapping matches in the string.\n",
        "        \n",
        "        If one or more groups are present in the pattern, return a\n",
        "        list of groups; this will be a list of tuples if the pattern\n",
        "        has more than one group.\n",
        "        \n",
        "        Empty matches are included in the result.\n",
        "    \n",
        "    finditer(pattern, string, flags=0)\n",
        "        Return an iterator over all non-overlapping matches in the\n",
        "        string.  For each match, the iterator returns a match object.\n",
        "        \n",
        "        Empty matches are included in the result.\n",
        "    \n",
        "    match(pattern, string, flags=0)\n",
        "        Try to apply the pattern at the start of the string, returning\n",
        "        a match object, or None if no match was found.\n",
        "    \n",
        "    purge()\n",
        "        Clear the regular expression cache\n",
        "    \n",
        "    search(pattern, string, flags=0)\n",
        "        Scan through string looking for a match to the pattern, returning\n",
        "        a match object, or None if no match was found.\n",
        "    \n",
        "    split(pattern, string, maxsplit=0, flags=0)\n",
        "        Split the source string by the occurrences of the pattern,\n",
        "        returning a list containing the resulting substrings.\n",
        "    \n",
        "    sub(pattern, repl, string, count=0, flags=0)\n",
        "        Return the string obtained by replacing the leftmost\n",
        "        non-overlapping occurrences of the pattern in string by the\n",
        "        replacement repl.  repl can be either a string or a callable;\n",
        "        if a string, backslash escapes in it are processed.  If it is\n",
        "        a callable, it's passed the match object and must return\n",
        "        a replacement string to be used.\n",
        "    \n",
        "    subn(pattern, repl, string, count=0, flags=0)\n",
        "        Return a 2-tuple containing (new_string, number).\n",
        "        new_string is the string obtained by replacing the leftmost\n",
        "        non-overlapping occurrences of the pattern in the source\n",
        "        string by the replacement repl.  number is the number of\n",
        "        substitutions that were made. repl can be either a string or a\n",
        "        callable; if a string, backslash escapes in it are processed.\n",
        "        If it is a callable, it's passed the match object and must\n",
        "        return a replacement string to be used.\n",
        "    \n",
        "    template(pattern, flags=0)\n",
        "        Compile a template pattern, returning a pattern object\n",
        "\n",
        "DATA\n",
        "    DOTALL = 16\n",
        "    I = 2\n",
        "    IGNORECASE = 2\n",
        "    L = 4\n",
        "    LOCALE = 4\n",
        "    M = 8\n",
        "    MULTILINE = 8\n",
        "    S = 16\n",
        "    U = 32\n",
        "    UNICODE = 32\n",
        "    VERBOSE = 64\n",
        "    X = 64\n",
        "    __all__ = ['match', 'search', 'sub', 'subn', 'split', 'findall', 'comp...\n",
        "    __version__ = '2.2.1'\n",
        "\n",
        "VERSION\n",
        "    2.2.1\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print re.split('x+', 'axxxxbxxxxc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['a', 'b', 'c']\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print re.split('x', 'axxxxbxxxxc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['a', '', '', '', 'b', '', '', '', 'c']\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print re.split('\\+\\+', 'hi+++there')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['hi', '+there']\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print re.split('([\\W\\s]|t)', \"what's up?\")"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['wha', 't', '', \"'\", 's', ' ', 'up', '?', '']\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "text = \"A first-class ticket to the U.S.A. isn't expensive?\"\n",
      "print re.split(' ', text)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'first-class', 'ticket', 'to', 'the', 'U.S.A.', \"isn't\", 'expensive?']\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "How to remove punctuation?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print re.split('\\W+', text)           # \\W=not a word character; +=1 or more"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'first', 'class', 'ticket', 'to', 'the', 'U', 'S', 'A', 'isn', 't', 'expensive', '']\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print re.findall('\\w+', text)         # \\w=a word character [a-zA-Z0-9_]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'first', 'class', 'ticket', 'to', 'the', 'U', 'S', 'A', 'isn', 't', 'expensive']\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# group punctuation with following letters\n",
      "print re.findall('\\w+|\\S\\w*', text)  # \\S=not a space; |=OR"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'first', '-class', 'ticket', 'to', 'the', 'U', '.S', '.A', '.', 'isn', \"'t\", 'expensive', '?']\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "How to keep hyphenated words and contractions together?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print re.findall(\"\\w+(?:[-']\\w+)*|[-.(]+|\\S\\w*\", text)\n",
      "# (?: specifies what to match, not what to capture"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'first-class', 'ticket', 'to', 'the', 'U', '.', 'S', '.', 'A', '.', \"isn't\", 'expensive', '?']\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print re.findall(\"(?:[A-Z]\\.)+|\\w+(?:[-']\\w+)*|[-.(]+|\\S\\w*\", text)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['A', 'first-class', 'ticket', 'to', 'the', 'U.S.A.', \"isn't\", 'expensive', '?']\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Normalization\n",
      "\n",
      "**nor\u00b7mal\u00b7iz\u00b7a\u00b7tion** (/\u02c8n\u00f4rm\u0259\u02ccliz\u0101-sh\u0259n/) *n.*\n",
      "\n",
      "1. The process of clustering *types* into *terms*.\n",
      "\n",
      "    Issues include: removing common words, special characters, casing, morphology"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Normalization: Stop words\n",
      "\n",
      "- Exclude common words\n",
      "  - *the*, *a*, *be*\n",
      "- Why?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "  - save space (length of postings list is huge!)\n",
      "  - no semantic content (?!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "*\"[to be or not to be](https://www.google.com/search?q=to+be+or+not+to+be&oq=to+be+or+not+to+be)\"* is all stop words!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Accents/Diacritics\n",
      "\n",
      "- naive vs. na\u00efve\n",
      "- pena (sorrow) vs pe\u00f1a (cliff)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- What will users enter?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Case\n",
      "\n",
      "- Typically, just convert everything to lowercase.\n",
      "- E.g., search Google for [CAT -cat](https://www.google.com/search?q=CAT+-cat)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Stemming / Lemmatizing\n",
      "\n",
      "**mor\u00b7phol\u00b7o\u00b7gy** (/m\u00f4r\u02c8f\u00e4l\u0259j\u0113/) *n.*\n",
      "\n",
      "1. (*Linguistics*) The study of the rules governing how words may take different forms in a language.\n",
      "\n",
      "*E.g.* \n",
      "\n",
      "- Pluralization: *dog* $\\xrightarrow{pluralize}$ *dogs* ; *goose* $\\xrightarrow{pluralize}$ *geese*\n",
      "\n",
      "\n",
      "- Tense: *play* $\\xrightarrow{past.tense}$ *played* ; *go* $\\xrightarrow{past.tense}$ *went*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**stem** (/stem/) *v.*\n",
      "\n",
      "1. To normalize based on crude morphology heuristics.\n",
      "\n",
      "    *E.g. remove all \"-s\" and \"-ed\" suffixes*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**lem\u00b7ma\u00b7tize** (\u02c8lem\u0259\u02cct\u012bz/) *v.*\n",
      "\n",
      "1. To create equivalence classes of word types using the morphological rules of a language.\n",
      "\n",
      "    *Often relies on **part-of-speech** tagging to select rules*.\n",
      "\n",
      "    *E.g. if * bed * is a noun, then do not remove * -ed *suffix.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Simple stemmer\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = 'hello'\n",
      "print s[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hello\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def stem(word):\n",
      "    for suffix in ['ies', 's', 'ed', 'ing']: # order matters!\n",
      "        if word.endswith(suffix):\n",
      "            return word[:-len(suffix)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**What can go wrong?**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Stemming Errors\n",
      "\n",
      "- **over-stemming**: merge types that should not be merged.\n",
      "- **under-stemming**: fail to merge types that should be merged."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "types = ['tied', 'ties', 'tis', 'bed', 'cities']\n",
      "print '\\n'.join([stem(w) for w in types])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ti\n",
        "t\n",
        "ti\n",
        "b\n",
        "cit\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**How does this affect search?**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Porter Stemmer\n",
      "\n",
      "- Very commonly used stemmer with a complex set of heuristics."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk.stem import PorterStemmer # See nltk.org (`pip install nltk`)\n",
      "porter = PorterStemmer()\n",
      "print types\n",
      "print '\\n'.join([porter.stem(x) for x in types])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['tied', 'ties', 'tis', 'bed', 'cities']\n",
        "tie\n",
        "tie\n",
        "ti\n",
        "bed\n",
        "citi\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print porter.stem('city')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "citi\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "types = ['bed', 'kiss',\n",
      "         'tied', 'tis',\n",
      "         'universal', 'university',\n",
      "         'experiment', 'experience',\n",
      "         'past', 'paste',\n",
      "         'alumnus', 'alumni',\n",
      "         'adhere', 'adhesion',\n",
      "         'create', 'creation']\n",
      "porter_results = [porter.stem(x) for x in types]\n",
      "print '\\n'.join(porter_results)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "bed\n",
        "kiss\n",
        "tie\n",
        "ti\n",
        "univers\n",
        "univers\n",
        "experi\n",
        "experi\n",
        "past\n",
        "past\n",
        "alumnu\n",
        "alumni\n",
        "adher\n",
        "adhes\n",
        "creat\n",
        "creation\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# WordNet Lemmatizer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "# See description: https://wordnet.princeton.edu/wordnet/man/morphy.7WN.html\n",
      "lemm = WordNetLemmatizer()\n",
      "lemm_results = [lemm.lemmatize(x) for x in types]\n",
      "print 'type, porter, lemmatizer\\n'\n",
      "print '\\n'.join([str(t) for t in zip(types, porter_results, lemm_results)])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "type, porter, lemmatizer\n",
        "\n",
        "('bed', u'bed', 'bed')\n",
        "('kiss', u'kiss', 'kiss')\n",
        "('tied', u'tie', 'tied')\n",
        "('tis', u'ti', u'ti')\n",
        "('universal', u'univers', 'universal')\n",
        "('university', u'univers', 'university')\n",
        "('experiment', u'experi', 'experiment')\n",
        "('experience', u'experi', 'experience')\n",
        "('past', u'past', 'past')\n",
        "('paste', u'past', 'paste')\n",
        "('alumnus', u'alumnu', 'alumnus')\n",
        "('alumni', u'alumni', u'alumnus')\n",
        "('adhere', u'adher', 'adhere')\n",
        "('adhesion', u'adhes', 'adhesion')\n",
        "('create', u'creat', 'create')\n",
        "('creation', u'creation', 'creation')\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print lemm.lemmatize('are')\n",
      "print lemm.lemmatize('is')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "are\n",
        "is\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print lemm.lemmatize('are', 'v')\n",
      "print lemm.lemmatize('is', 'v')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "be\n",
        "be\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# A principled approach?\n",
      "\n",
      "Given the many number of ways to preprocess text, how do we know which one is best?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Approaches:\n",
      "\n",
      "- Assume types that appear in similar contexts can be merged.\n",
      "  - e.g., *universally* and *universal* appear in similar documents, but not *university*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Learn from user behavior\n",
      "  - e.g., users click on very different search results if they search for *universal* vs *university*.\n",
      "  \n",
      "We'll explore both later in the course."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}