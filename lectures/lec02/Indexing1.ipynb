{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# CS 429: Information Retrieval\n",
      "\n",
      "<br>\n",
      "## Lecture 2: Indexing\n",
      "\n",
      "<br>\n",
      "\n",
      "### Dr. Aron Culotta\n",
      "### Illinois Institute of Technology \n",
      "### Spring 2014\n",
      "\n",
      "---\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Indexing Pipeline\n",
      "\n",
      "1. Collect documents\n",
      "2. Tokenize\n",
      "3. Normalize\n",
      "4. Index"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Indexing Pipeline\n",
      "\n",
      "document $\\xrightarrow{tokenize}$ (tokens, types) $\\xrightarrow{normalize}$ terms $\\xrightarrow{index}$ inverted index"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 1. Collect documents.\n",
      "document = \"I didn't know where i was\"\n",
      "\n",
      "# 2. Tokenize\n",
      "tokens = [\"I\", \"didn't\", \"know\", \"where\", \"i\", \"was\"]\n",
      "types = [\"I\", \"didn't\", \"know\", \"where\", \"was\"] # unique tokens.\n",
      "\n",
      "# 3. Normalize\n",
      "# remove common words like 'was'\n",
      "terms = [\"i\", \"didnt\", \"know\", \"where\"]\n",
      "\n",
      "# 4. Index\n",
      "index = {'i': [0],\n",
      "         'didnt': [0],\n",
      "         'know' : [0],\n",
      "         'where' : [0]}"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "**e\u00b7quiv\u00b7a\u00b7lence class** (/i'kwiv\u0259l\u0259ns klas/) *n.*\n",
      "\n",
      "1. A subset whose elements are equivalent according to some relation $\\sim$ .\n",
      "\n",
      "    $S' \\subseteq S \\hspace{.2cm}$ s.t. $\\hspace{.2cm}x_i \\sim x_j \\hspace{.1cm} \\forall x_i, x_j \\in S'$\n",
      "    \n",
      "    E.g., consider the set \\{dog, cat, spider\\} and the relation \"has same number of legs\". Then there are two equivalence classes: \\{dog, cat\\} and \\{spider\\}."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "**to\u00b7ken** (/t\u014dk\u0259n/) *n*.\n",
      "\n",
      "1. A sequence of characters in a document that form a meaningful unit.\n",
      "2. The output of a *tokenizer*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**type** (/t\u012bp/) *n.*\n",
      "\n",
      "1. An equivalence class of *tokens* under the relation \"have the same characters.\"\n",
      "2. By analogy to OO-programming, class:object :: type:token\n",
      "\n",
      "    *e.g., \"to be or not to be\"* $\\xrightarrow{tokenize}$ *tokens={to, be, or, not, to be}* $\\hspace{.2cm}$  *types={to, be, or, not}*\n",
      "    \n",
      "    The type 'to' is an equivalence class containing the first and fifth tokens of the document.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "**term** (/t\u0259rm/) *n.*\n",
      "\n",
      "1. An equivalence class of *types* under the relation \"have same normalized form.\"\n",
      "2. The keys in the inverted index.\n",
      "\n",
      "    e.g., types={John, john, aren't, arent} $\\xrightarrow{normalize}$ terms={john, arent}.\n",
      "\n",
      "    The term \"john\" is an equivalence class containing types {John, john}.\n",
      "    \n",
      "    The term \"arent\" is an equivalence class containing types {arent, aren't}.\n",
      "    \n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Tokenization: Punctuation\n",
      "\n",
      "- [CAR](https://www.google.com/search?q=CAR) vs [C.A.R](https://www.google.com/search?q=C.A.R.).\n",
      "- O'Neill vs ONeill vs O Neill"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Tokenization: Compound Nouns\n",
      "\n",
      "San Francisco"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Tokenization: Segmentation\n",
      "\n",
      "- *Lebensversicherungsgesellschaftsangestellter*\n",
      "  - \"life insurance company employee\"\n",
      "  \n",
      "  \n",
      "- \u6211\u4e0d\u80fd\u8bf4\u4e2d\u56fd\u8bdd"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Normalization\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Normalization: Stop words\n",
      "\n",
      "- Exclude common words\n",
      "  - *the*, *a*, *be*\n",
      "- Why?\n",
      "  - save space (length of postings list is huge!)\n",
      "  - no semantic content (?!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "*\"[to be or not to be](https://www.google.com/search?q=to+be+or+not+to+be&oq=to+be+or+not+to+be)\"* is all stop words!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Accents/Diacritics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Case"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Stemming / Lemmatizing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Stemming Errors\n",
      "\n",
      "- **over-stemming**: merge types that should not be merged.\n",
      "- **under-stemming**: fail to merge types that should be merged."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stem(word):\n",
      "    for suffix in ['s', 'ed', 'ing', 'ies']:\n",
      "        if word.endswith(suffix):\n",
      "            return word[:-len(suffix)]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print [stem(w) for w in ['tied', 'tis']]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['ti', 'ti']\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import PorterStemmer\n",
      "porter = PorterStemmer()\n",
      "print [porter.stem(x) for x in ['tied', 'tis',\n",
      "                                'universal', 'university', # over-stemming\n",
      "                                'experiment', 'experience',\n",
      "                                'alumnus', 'alumni',  # under-stemming\n",
      "                                'adhere', 'adhesion',\n",
      "                                ]]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['tie', 'ti', 'univers', 'univers', 'experi', 'experi', 'alumnu', 'alumni', 'adher', 'adhes']\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Language Identification"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}