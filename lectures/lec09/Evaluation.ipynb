{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CS 429: Information Retrieval\n",
    "\n",
    "<br>\n",
    "\n",
    "## Lecture 9: Evaluation\n",
    "\n",
    "<br>\n",
    "\n",
    "### Dr. Aron Culotta\n",
    "### Illinois Institute of Technology \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Last time\n",
    "\n",
    "- Scalable cosine similarity ranking\n",
    "\n",
    "Today\n",
    "\n",
    "- Is our search engine any good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "- Your first day at Bing, your boss says \"make our search engine better than Google's\"\n",
    "\n",
    "\n",
    "- How do you know when you're done?\n",
    "- Maybe it's already better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to measure goodness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Relevant results\n",
    "2. Speed (indexing and search)\n",
    "3. Query capabilities\n",
    "4. Pleasing user interface\n",
    "5. User is *happy*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to tell when the user is happy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- They come back.\n",
    "- They come back frequently.\n",
    "- They take surveys and tell you they like you.\n",
    "\n",
    "\n",
    "- We'll settle for measuring **relevance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is relevance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A document is **relevant** if it addresses the user's information need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is **information need**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The user's information need is the explicit definition of what they are looking for.\n",
    "- The query is only a proxy for that need.\n",
    "\n",
    "\n",
    "- E.g. the **information need** may be \"find a flight from Chicago to Detroit leaving tomorrow\"\n",
    "   - The **query** may be \"ORD to DTW\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How relevant are the results of your search engine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An **Information Retrieval Benchmark** consists of: \n",
    "  1. A documents collection.\n",
    "  2. A set of queries (and corresponding information needs).\n",
    "  3. A set of **relevance judgements** for each query-document pair (usually binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Document collection:**\n",
    "\n",
    "- Which documents?\n",
    "- How many?\n",
    "\n",
    "**Queries:**\n",
    "\n",
    "- Which queries?\n",
    "- How many?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Generally, want a diverse set of queries that people actually use, over a diverse set of documents that people actually read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Relevance judgements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Who is making these judgements?\n",
    "- How many do we need?\n",
    "- How do we know they are right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we have 10M documents and 1K queries:\n",
    "  - 10B relevance judgements??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Crowd-sourcing can help, but not solve.\n",
    "- Either consider small datasets (<10K docs)\n",
    "- Or only judge a subset\n",
    "  - E.g., only consider those documents in top $K$ by some reasonable baseline system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Are the humans right?\n",
    "\n",
    "- How do we know we can trust the human relevance judgements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Make multiple people label the same document.\n",
    "- **Inter-annotator agreement:** How often do two humans agree on the label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- E.g., consider two humans labeling 100 documents:\n",
    "\n",
    "<table>\n",
    "<tr><td> </td> <td> </td> <td colspan=2> **Person 1** </td> </tr>\n",
    "<tr><td> </td> <td> </td> <td> Relevant </td> <td> Not Relevant </td> </tr>\n",
    "<tr><td rowspan=2> **Person 2** </td> <td> Relevant </td> <td> 50 </td> <td> 20 </td> </tr>\n",
    "<tr>                              <td> Not Relevant </td> <td> 10 </td> <td> 20 </td> </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "- Simple **agreement**: fraction of documents with matching labels. $\\frac{70}{100} = 70\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, how much agreement would we expect by chance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Person 1 says Relevant $60\\%$ of the time.\n",
    "- Person 2 says Relevant $70\\%$ of the time.\n",
    "- Chance that they both say relevant at the same time? $60\\% \\times 70\\% = 42\\%$.\n",
    "\n",
    "\n",
    "- Person 1 says Not Relevant $40\\%$ of the time.\n",
    "- Person 2 says Not Relevant $30\\%$ of the time.\n",
    "- Chance that they both say not relevant at the same time? $40\\% \\times 30\\% = 12\\%$.\n",
    "\n",
    "\n",
    "- Chance that they agree on any document (both say yes or both say no): $42\\% + 12\\% = 54\\%$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "** Cohen's Kappa ** $\\kappa$\n",
    "\n",
    "- Percent agreement beyond that expected by chance\n",
    "\n",
    "$ \\kappa = \\frac{P(A) - P(E)}{1 - P(E)}$\n",
    "\n",
    "- $P(A)$ = simple agreement proportion\n",
    "- $P(E)$ = agreement proportion expected by chance\n",
    "\n",
    "\n",
    "E.g., $\\kappa = \\frac{.7 - .54}{1 - .54} = .3478$\n",
    "\n",
    "- $k=0$ if no better than chance, $k=1$ if perfect agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assuming humans are right...\n",
    "\n",
    "Given document collection, a query, and relevance judgements, what score do we give to our search engine?\n",
    "\n",
    "- Assume search engine returns $K$ results\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr> <td>                   </td>  <td> **Relevant**  </td>  <td> **Nonrelevant** </td> </tr>\n",
    "<tr> <td> **Retrieved**     </td>  <td> true pos (**tp**)      </td>  <td> false pos (**fp**)   </td> </tr>\n",
    "<tr> <td> **Not Retrieved** </td>  <td> false neg (**fn**)     </td>  <td> true neg (**tn**)    </td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Example:\n",
    "\n",
    "\n",
    "```\n",
    "Rank    DocID    Relevant\n",
    "----    -----    --------\n",
    "1       123      Y\n",
    "2       456      N\n",
    "3       789      N\n",
    "4       321      Y\n",
    "5       654      N\n",
    "```\n",
    "\n",
    "Assume there are 10 relevant documents total and 100 documents in the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr> <td>                   </td>  <td> **Relevant**  </td>  <td> **Nonrelevant** </td> </tr>\n",
    "<tr> <td> **Retrieved**     </td>  <td> 2 (**tp**)      </td>  <td> 3 (**fp**)   </td> </tr>\n",
    "<tr> <td> **Not Retrieved** </td>  <td> 8 (**fn**)     </td>  <td> 87 (**tn**)    </td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Accuracy:** $\\frac{tp + tn}{tp + fn + fp + tn}$\n",
    "\n",
    "e.g., $\\frac{2 + 87}{2 + 8 + 3 + 87} = 89\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why is this a mostly useless measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Very few documents are relevant, so returning 0 documents has nearly perfect true negatives.\n",
    "\n",
    "<table>\n",
    "<tr> <td>                   </td>  <td> **Relevant**  </td>  <td> **Nonrelevant** </td> </tr>\n",
    "<tr> <td> **Retrieved**     </td>  <td> 0 (**tp**)      </td>  <td> 0 (**fp**)   </td> </tr>\n",
    "<tr> <td> **Not Retrieved** </td>  <td> 10 (**fn**)     </td>  <td> 90 (**tn**)    </td> </tr>\n",
    "</table>\n",
    "\n",
    "**Accuracy:** $\\frac{tp + tn}{tp + fn + fp + tn}$\n",
    "\n",
    "e.g., $\\frac{0 + 90}{0 + 10 + 0 + 90} = 90\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Precision and Recall\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr> <td>                   </td>  <td> **Relevant**  </td>  <td> **Nonrelevant** </td> </tr>\n",
    "<tr> <td> **Retrieved**     </td>  <td> true pos (**tp**)      </td>  <td> false pos (**fp**)   </td> </tr>\n",
    "<tr> <td> **Not Retrieved** </td>  <td> false neg (**fn**)     </td>  <td> true neg (**tn**)    </td> </tr>\n",
    "</table>\n",
    "\n",
    "- **Precision:** $P = \\frac{tp}{tp + fp}$,  The fraction of *returned* documents that are *relevant*.\n",
    "\n",
    "\n",
    "- **Recall:** &nbsp;&nbsp;&nbsp; $R = \\frac{tp}{tp + fn}$,  The fraction of *relevant* documents that are *returned*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table>\n",
    "<tr> <td>                   </td>  <td> **Relevant**  </td>  <td> **Nonrelevant** </td> </tr>\n",
    "<tr> <td> **Retrieved**     </td>  <td> 2 (**tp**)      </td>  <td> 3 (**fp**)   </td> </tr>\n",
    "<tr> <td> **Not Retrieved** </td>  <td> 8 (**fn**)     </td>  <td> 87 (**tn**)    </td> </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$P = \\frac{tp}{tp + fp} = \\frac{2}{2 + 3} = 40\\%$\n",
    "\n",
    "$R = \\frac{tp}{tp + fn} = \\frac{2}{2 + 8} = 20\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "<tr> <td>                   </td>  <td> **Relevant**  </td>  <td> **Nonrelevant** </td> </tr>\n",
    "<tr> <td> **Retrieved**     </td>  <td> 0 (**tp**)      </td>  <td> 0 (**fp**)   </td> </tr>\n",
    "<tr> <td> **Not Retrieved** </td>  <td> 10 (**fn**)     </td>  <td> 90 (**tn**)    </td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$P = \\frac{tp}{tp + fp} = \\frac{0}{0} = NaN \\rightarrow 0\\%$\n",
    "\n",
    "$R = \\frac{tp}{tp + fn} = \\frac{0}{10} = 0\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How to combine precision and recall?\n",
    "\n",
    "**Harmonic mean** of $P$ and $R$ is $\\frac{1}{\\frac{1}{P} + \\frac{1}{R}}$\n",
    "\n",
    "**Weighted harmonic mean:**  $\\frac{1}{\\alpha\\frac{1}{P} + (1-\\alpha)\\frac{1}{R}}$\n",
    "\n",
    "- Greater $\\alpha \\rightarrow$ precision is more important than recall\n",
    "\n",
    "**F1:** Setting $\\alpha=\\frac{1}{2}$ leads to:  $\\frac{2\\cdot P \\cdot R}{P + R}$\n",
    "\n",
    "- **F1** commonly used; weights precision and recall equally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why didn't we just use the arithmetic mean? ($\\frac{P + R}{2}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "# Compare arithmetic mean of precision and recall to F1 for fixed precision of 50%.\n",
    "\n",
    "def arith_mean(p, r):\n",
    "    return (p + r) / 2.0\n",
    "\n",
    "def f1(p, r):\n",
    "    return (2.0 * p * r) / (p + r)\n",
    "\n",
    "p = .5\n",
    "r = [0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]\n",
    "\n",
    "xlabel('recall')\n",
    "ylabel('mean')\n",
    "plot(r, [arith_mean(p, ri) for ri in r], label='arithmetic')\n",
    "plot(r, [f1(p, ri) for ri in r], label='f1')\n",
    "legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Precision-Recall Curves\n",
    "\n",
    "- Depending on use-case, precision or recall may be more important\n",
    "  - Lawyer who cannot miss a single \"smoking-gun\" document needs 100% recall.\n",
    "  - Average web user doesn't want **all** documents with the song lyrics she's searching for.\n",
    "- Precision-recall curves displays results varying the number of returned documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "Rank    DocID    Relevant\n",
    "----    -----    --------\n",
    "1       123      Y\n",
    "2       456      N\n",
    "3       789      N\n",
    "4       321      Y\n",
    "5       654      N\n",
    "```\n",
    "\n",
    "Assume there are 10 relevant documents total and 100 documents in the collection.\n",
    "\n",
    "$P = \\frac{tp}{tp + fp} = \\frac{2}{2 + 3} = 40\\%$\n",
    "\n",
    "$R = \\frac{tp}{tp + fn} = \\frac{2}{2 + 8} = 20\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "Rank    DocID    Relevant\n",
    "----    -----    --------\n",
    "1       123      Y\n",
    "2       456      N\n",
    "3       789      N\n",
    "4       321      Y\n",
    "5       654      N\n",
    "--\n",
    "6       987      Y\n",
    "7       135      N\n",
    "8       246      N\n",
    "9       357      N\n",
    "10      468      N\n",
    "```\n",
    "\n",
    "<table>\n",
    "<tr> <td>                   </td>  <td> **Relevant**  </td>  <td> **Nonrelevant** </td> </tr>\n",
    "<tr> <td> **Retrieved**     </td>  <td> <del>2</del> &nbsp; 3 (**tp**)      </td>  <td> <del>3</del> &nbsp;&nbsp; 7 (**fp**)   </td> </tr>\n",
    "<tr> <td> **Not Retrieved** </td>  <td> <del>8</del> &nbsp; 7 (**fn**)     </td>  <td> <del>87</del> &nbsp; 83 (**tn**)    </td> </tr>\n",
    "</table>\n",
    "\n",
    "$P = \\frac{tp}{tp + fp} = \\frac{3}{3 + 7} = 30\\%$\n",
    "\n",
    "$R = \\frac{tp}{tp + fn} = \\frac{3}{3 + 7} = 30\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xlabel('recall')\n",
    "ylabel('precision')\n",
    "plot([.2, .3], [.4, .3], 'bo')\n",
    "xlim((0, .4))\n",
    "ylim((0, 1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Precision and recall at each rank:\n",
    "\n",
    "```\n",
    "Rank    DocID    Relevant   P      R\n",
    "----    -----    --------   ----   ----\n",
    "1       123      Y          1.0    0.1\n",
    "2       456      N          0.5    0.1\n",
    "3       789      N          0.33   0.1  \n",
    "4       321      Y          0.5    0.2\n",
    "5       654      N          0.4    0.2\n",
    "6       987      Y          0.5    0.3\n",
    "7       135      N          0.43   0.3\n",
    "8       246      N          0.375  0.3\n",
    "9       357      N          0.33   0.3\n",
    "10      468      N          0.3    0.3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# compute precision/recall at each sublist of size 1 to 10\n",
    "xlabel('recall')\n",
    "ylabel('precision')\n",
    "precisions = [1, .5, .33, .5, .4, .5, .43, .375, .33, .3]\n",
    "recalls = [.1, .1, .1, .2, .2, .3, .3, .3, .3, .3]\n",
    "plot(recalls, precisions, 'bo')\n",
    "xlim((0, .4))\n",
    "ylim((0, 1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Interpolated precision: max of precisions to right of value\n",
    "xlabel('recall')\n",
    "ylabel('precision')\n",
    "interpolated_pre = [max(precisions[i:]) for i in range(len(precisions))]\n",
    "print interpolated_pre\n",
    "step(recalls, interpolated_pre, 'bo')\n",
    "xlim((0, .4))\n",
    "ylim((0, 1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mean Average Precision (MAP)\n",
    "\n",
    "- Average the precision values for top $k$ documents, considering only those elements where a relevant document is found.\n",
    "\n",
    "|Rank|    DocID  |    Relevant |   P       |   R   |\n",
    "|----|    -----  |    -------- |  ----     |  ---- |\n",
    "| 1  |       123 |     Y       |   **1.0** |   0.1 |\n",
    "| 2  |       456 |     N       |   0.5     |   0.1 |\n",
    "| 3  |       789 |     N       |   0.33    |   0.1 |  \n",
    "| 4  |       321 |     Y       |   **0.5** |   0.2 |\n",
    "| 5  |       654 |     N       |   0.4     |   0.2 |\n",
    "| 6  |       987 |     Y       |   **0.5** |   0.3 |\n",
    "| 7  |       135 |     N       |   0.43    |   0.3 |\n",
    "| 8  |       246 |     N       |   0.375   |   0.3 |\n",
    "| 9  |       357 |     N       |   0.33    |   0.3 |\n",
    "| 10 |       468 |     N       |   0.3     |   0.3 |\n",
    "\n",
    "If relevant document not returned, assume 0 precision for those. So, if there are 10 relevant documents, MAP is:\n",
    "\n",
    "$MAP=\\frac{1.0 + 0.5 + 0.5 + 0 + \\ldots + 0}{10} = .2$\n",
    "\n",
    "If there are 5 relevant documents, MAP is:\n",
    "\n",
    "$MAP=\\frac{1.0 + 0.5 + 0.5 + 0 + \\ldots + 0}{5} = .4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# R-Precision\n",
    "\n",
    "Precision considering the top $R$ documents, where $R$ is the number of relevant documents.\n",
    "\n",
    "In our example, precision \"at 10\" ($P$@$10$) is $0.3$\n",
    "\n",
    "Perfect system has $R$-precision of 1.0 (e.g., top $R$ results are all relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Labeling data is hard...\n",
    "\n",
    "Up to now, we've assumed a human has annotated many documents by relevance.\n",
    "\n",
    "Can we get this data with less effort?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For query $q$, the system returns\n",
    "\n",
    "```\n",
    "Rank    DocID\n",
    "----    -----\n",
    "1       123\n",
    "2       456\n",
    "3       789\n",
    "```\n",
    "\n",
    "Click-through data:\n",
    "\n",
    "```\n",
    "DocID   Clicks\n",
    "-----   ------\n",
    "123     1,000\n",
    "456     500\n",
    "789     100\n",
    "```\n",
    "\n",
    "Does this mean that users think document 123 is more relevant than document 456?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Positional bias\n",
    "\n",
    "Users are *a-priori* more likely to click a higher ranked document.\n",
    "\n",
    "How can we compensate for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Pairwise preferences:**\n",
    "\n",
    "```\n",
    "DocID   Clicks\n",
    "-----   ------\n",
    "123     1,000\n",
    "456     500\n",
    "789     5,000\n",
    "```\n",
    "\n",
    "Can probably conclude that 789 > 456 and 789 > 123\n",
    "  - otherwise, user would have clicked the earlier link\n",
    "\n",
    "How can we compute a score from this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $L^*$ be the set of **true** pairwise preferences $\\{d_i > d_j\\}$\n",
    "\n",
    "Let $L$ be the ranking produced by our system: $d_{123} > d_{456} > d_{789} \\ldots$\n",
    "\n",
    "Let $r_L(i, j)$ be 1 if ranking $L$ ranks $i$ before $j$.\n",
    "\n",
    "**Kendall tau distance** is the number of pairwise disagreements: \n",
    "\n",
    "$\\tau(L, L^*) = {\\Big|}{\\big \\{}(i,j) $ s.t. $r_L(i, j) \\ne r_{L^*}(i, j){\\big \\}}{\\Big|}$\n",
    "\n",
    "If perfectly matched: $\\tau=0$; if perfectly mismatched: $\\tau=\\Big| L^* \\Big|$\n",
    "\n",
    "E.g., \n",
    "\n",
    "- true list &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$=[2, 1, 3]$\n",
    "- predicted list $=$ $[1,2,3]$\n",
    "\n",
    "- $L^*= (2>1,$ &nbsp; $ 2>3,$ &nbsp; $ 1>3)$\n",
    "- $L$ &nbsp;&nbsp; $= (1>2,$ &nbsp; $ 1>3,$ &nbsp; $ 2>3)$\n",
    "- $\\tau(L, L^*) = \\Big|(1,2)\\Big| = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interleaving rankings\n",
    "\n",
    "Given two ranking systems A and B, which is better?\n",
    "\n",
    "Interleave rankings from each, showing half with A first, half with B first.\n",
    "\n",
    "Group 1: $A_1$, $B_1$, $A_2$, $B_2 \\ldots$\n",
    "\n",
    "Group 2: $B_1$, $A_1$, $B_2$, $A_2 \\ldots$\n",
    "\n",
    "E.g., if $A_1$ is clicked more than $B_1$ in Group 2, then A might be better than B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A/B Testing\n",
    "\n",
    "A method of measuring the impact of a system parameter.\n",
    "\n",
    "- E.g., Should I stem? What should the size of the champion list be? How should I tokenize?\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "To do A/B testing:\n",
    "\n",
    "\n",
    "1. Create a search engine with millions of users.\n",
    "2. Divert a small sample (1%) to the *experimental* search engine with that feature modified.\n",
    "3. Measure difference in user happiness between groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Happiness:\n",
    "  - proportion of time first result is clicked\n",
    "  - proportion of time top $k$ result is clicked  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
