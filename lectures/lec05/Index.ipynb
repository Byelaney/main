{
 "metadata": {
  "name": "",
  "signature": "sha256:477d2e354b7c13267692c4af906b520ef4255ec52bcba65aa7fdb1bc6a2c6313"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# CS 429: Information Retrieval\n",
      "\n",
      "<br>\n",
      "\n",
      "## Lecture 5: Scalable Indexing\n",
      "\n",
      "<br>\n",
      "\n",
      "### Dr. Aron Culotta\n",
      "### Illinois Institute of Technology \n",
      "### Spring 2015\n",
      "\n",
      "---\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Last time:\n",
      "\n",
      "- Efficient retrieval of postings lists\n",
      "- Wildcard queries\n",
      "- Spelling correction\n",
      "\n",
      "Today: \n",
      "\n",
      "- How do we build an index that does not fit into memory? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Building an index\n",
      "\n",
      "- Up to now, we've assumed everything fits in memory.\n",
      "- We'll discuss three ways to scale\n",
      "  1. Block sort-based indexing (**BSBI**)\n",
      "  2. Single-pass in-memory indexing (**SPIMI**)\n",
      "  3. MapReduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "How long does it take to read 100MB from disk?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- **Seek time** (to locate data)\n",
      "- **Transfer rate** (to copy from disk into memory)\n",
      "- Contiguous or non-contiguous?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Block sort-based indexing (BSBI)\n",
      "\n",
      "Assume a single machine.\n",
      "\n",
      "1. Split documents into **blocks**\n",
      "2. For each block:\n",
      "  1. Parse each block into (word_id, doc_id) pairs\n",
      "  2. Sort pairs and create separate postings lists for each block.\n",
      "  3. Write postings lists to disk\n",
      "3. Merge the postings lists file for each block\n",
      "\n",
      "![bsbi](files/bsbi.png)\n",
      "\n",
      "(source: [MRS](http://nlp.stanford.edu/IR-book/pdf/04const.pdf))"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "d = defaultdict(lambda: len(d))\n",
      "d['zebra']\n",
      "d['cat']\n",
      "d['zebra']\n",
      "print d\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "defaultdict(<function <lambda> at 0x110e816e0>, {'zebra': 0, 'cat': 1})\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# BSBI: Create one postings list per block of documents.\n",
      "from collections import defaultdict\n",
      "from itertools import groupby\n",
      "\n",
      "block1 = [(0, [\"the\", \"dog\", \"jumped\"]),\n",
      "          (1, [\"the\", \"cat\", \"jumped\"])]\n",
      "\n",
      "block2 = [(2, [\"a\", \"dog\", \"ran\"]),\n",
      "          (3, [\"the\", \"zebra\", \"jumped\"])]\n",
      "\n",
      "blocks = [block1, block2]\n",
      "\n",
      "vocab = defaultdict(lambda: len(vocab))\n",
      "for block_id, block in enumerate(blocks):\n",
      "    # A. Collect all individual postings: (word_id, doc_id) pairs.\n",
      "    postings = []\n",
      "    for doc_id, doc in block:\n",
      "        for word in doc:\n",
      "            postings.append((vocab[word], doc_id))\n",
      "    print 'block', block_id, 'postings=', postings\n",
      "    print 'vocab=', vocab.items()\n",
      "    \n",
      "    # B. Sort postings and create postings lists.\n",
      "    postings = sorted(postings, key=lambda x: x[0])\n",
      "    print 'block', block_id, 'sorted postings=', postings\n",
      "    \n",
      "    # Group postings for same term together.\n",
      "    postings = groupby(postings, key=lambda x:x[0])\n",
      "    postings = [(word_id, [g[1] for g in group]) for word_id, group in postings]\n",
      "    print 'block', block_id, 'grouped postings=', postings\n",
      "    \n",
      "    # C. Write to disk\n",
      "    f = open('bsbi_block' + str(block_id) + '.txt', 'wt')\n",
      "    f.write('\\n'.join(['%s' % str(p) for p in postings]))\n",
      "    f.close()\n",
      "    print\n",
      "    \n",
      "# Then, merge blocks in linear time."
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "block 0 postings= [(0, 0), (1, 0), (2, 0), (0, 1), (3, 1), (2, 1)]\n",
        "vocab= [('jumped', 2), ('the', 0), ('dog', 1), ('cat', 3)]\n",
        "block 0 sorted postings= [(0, 0), (0, 1), (1, 0), (2, 0), (2, 1), (3, 1)]\n",
        "block 0 grouped postings= [(0, [0, 1]), (1, [0]), (2, [0, 1]), (3, [1])]\n",
        "\n",
        "block 1 postings= [(4, 2), (1, 2), (5, 2), (0, 3), (6, 3), (2, 3)]\n",
        "vocab= [('a', 4), ('ran', 5), ('jumped', 2), ('dog', 1), ('cat', 3), ('zebra', 6), ('the', 0)]\n",
        "block 1 sorted postings= [(0, 3), (1, 2), (2, 3), (4, 2), (5, 2), (6, 3)]\n",
        "block 1 grouped postings= [(0, [3]), (1, [2]), (2, [3]), (4, [2]), (5, [2]), (6, [3])]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Merging posting list blocks\n",
      "\n",
      "Since each block is sorted by term, can do a single linear pass through each block.\n",
      "\n",
      "- Open all postings files simulataneously.\n",
      "- Read small buffer from each.\n",
      "- Equivalent to a union of postings lists for same term."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# BSBI\n",
      "\n",
      "Space requirements?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Number of tokens in each block ($T$)\n",
      "- Number of unique terms in vocabulary ($V$)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Time requirements?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Sorting (word_id, doc_id) pairs in each block.\n",
      "- $O(T log T)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Single-pass in-memory indexing (SPIMI)\n",
      "\n",
      "- separate dictionary for each block\n",
      "- create postings lists on the fly, rather than collecting all postings then sorting.\n",
      "- sort postings lists, rather than individual postings"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# SPIMI: Create one postings list per block of documents.\n",
      "from collections import defaultdict\n",
      "from itertools import groupby\n",
      "\n",
      "block1 = [(0, [\"the\", \"dog\", \"jumped\"]),\n",
      "          (1, [\"the\", \"cat\", \"jumped\"])]\n",
      "\n",
      "block2 = [(2, [\"a\", \"dog\", \"ran\"]),\n",
      "          (3, [\"the\", \"zebra\", \"jumped\"])]\n",
      "\n",
      "blocks = [block1, block2]\n",
      "\n",
      "for block_id, block in enumerate(blocks):\n",
      "    # Note that there is a new vocab for each block!\n",
      "    vocab = defaultdict(lambda: len(vocab))  # maps from term -> term_id\n",
      "    index = defaultdict(lambda: [])          # from term_id -> postings list\n",
      "\n",
      "    for doc_id, doc in block:\n",
      "        # append doc_id to the postings list of each term\n",
      "        for word in doc:\n",
      "            index[vocab[word]].append(doc_id)\n",
      "    \n",
      "    \n",
      "    # B. Sort terms\n",
      "    sorted_terms = sorted(vocab.keys())\n",
      "    print 'Block', block_id, [t + ' ' + str(index[vocab[t]]) for t in sorted_terms]\n",
      "    \n",
      "    # C. Write to disk\n",
      "    f = open('spimi_block' + str(block_id) + '.txt', 'wt')\n",
      "    f.write('\\n'.join(['%d, %s' % (vocab[t], str(index[vocab[t]])) for t in sorted_terms]))\n",
      "    f.close()\n",
      "    print\n",
      "    print vocab\n",
      "    \n",
      "# Then, merge blocks in linear time."
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Block 0 ['cat [1]', 'dog [0]', 'jumped [0, 1]', 'the [0, 1]']\n",
        "\n",
        "defaultdict(<function <lambda> at 0x110e81de8>, {'jumped': 2, 'the': 0, 'dog': 1, 'cat': 3})\n",
        "Block 1 ['a [2]', 'dog [2]', 'jumped [3]', 'ran [2]', 'the [3]', 'zebra [3]']\n",
        "\n",
        "defaultdict(<function <lambda> at 0x110e818c0>, {'a': 0, 'ran': 2, 'jumped': 5, 'dog': 1, 'zebra': 4, 'the': 3})\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# SPIMI\n",
      "\n",
      "Space requirements?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Number of tokens in each block ($T$)\n",
      "- Number of unique terms in vocabulary *in each block* ($V_b << V$)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Time requirements?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Sorting unique terms in each block \n",
      "- $O(V log V)$\n",
      "  - compare to $O(T log T)$ for BSBI"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# MapReduce\n",
      "\n",
      "- What if we had 100K servers?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- **MapReduce:**\n",
      "  - A distributed programming framework\n",
      "  - Breaks large data into smaller data, called **splits**\n",
      "- Two phases:\n",
      "  - **Map:**\n",
      "    - Input: one split\n",
      "    - Output: (key, value) pairs\n",
      "  - **Reduce:**\n",
      "    - Input: (key, list of mapped values)\n",
      "    - Output: list of output values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# MapReduce Counting Example\n",
      "\n",
      "```\n",
      "map(key, value):\n",
      "  for each word in value:\n",
      "    output word, 1\n",
      "       \n",
      "reduce(key, values):\n",
      "  output key, sum(values)\n",
      "```\n",
      "\n",
      "Framework takes care of grouping keys together to call `reduce` appropriately.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# MapReduce Counting Example\n",
      "\n",
      "- Split 1: \"Twinkle, twinkle little star\"\n",
      "- Split 2: \"Little by little\"\n",
      "\n",
      "#### Map\n",
      "\n",
      "\"Twinkle, twinkle little star\" $\\rightarrow$ **Mapper 1**  $\\rightarrow$ (twinkle, 1), (twinkle, 1), (little, 1), (star, 1)\n",
      "\n",
      "\"Little by little\" $\\rightarrow$ **Mapper 2** $\\rightarrow$ (little, 1), (by, 1), (little, 1)\n",
      "\n",
      "#### Reduce\n",
      "\n",
      "(twinkle, 1), (twinkle, 1) $\\rightarrow$ **Reducer 1** $\\rightarrow$ (twinkle, 2)\n",
      "\n",
      "(little, 1), (little, 1), (little, 1) $\\rightarrow$ **Reducer 2** $\\rightarrow$ (little, 3)\n",
      "\n",
      "..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Indexing with MapReduce\n",
      "\n",
      "- **Map**: Read a document and output (term, doc_id) pairs.\n",
      "- **Reduce**: Read a list of doc_ids for a term and output a postings list.\n",
      "\n",
      "\n",
      "\"Twinkle, twinkle little star\" $\\rightarrow$ **Mapper 1**  $\\rightarrow$ (twinkle, 0), (little, 0), (star, 0)\n",
      "\n",
      "\"Little by little\" $\\rightarrow$ **Mapper 2** $\\rightarrow$ (little, 1), (by, 1)\n",
      "\n",
      "#### Reduce\n",
      "\n",
      "(little, 0), (little, 1) $\\rightarrow$ **Reducer 1** $\\rightarrow$ (little, [0, 1])\n",
      "\n",
      "..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import re\n",
      "from mrjob.job import MRJob  # install with `pip install mrjob`\n",
      "\n",
      "class MRIndexer(MRJob):\n",
      "        \n",
      "    def mapper(self, _, line):\n",
      "        # Emit word, doc_id pairs from lines that look like:\n",
      "        # doc_id [document tokens]\n",
      "        words = re.findall('\\w+', line.lower())\n",
      "        doc_id = int(words[0])\n",
      "        for word in set(words[1:]):\n",
      "            yield word, doc_id\n",
      "\n",
      "    def reducer(self, key, values):\n",
      "        # key is a term, values is an (unsorted) list of doc_ids\n",
      "        yield key, sorted(values)\n",
      "\n",
      "# Run python mr.py to execute this example."
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 3
    }
   ],
   "metadata": {}
  }
 ]
}