{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# CS 429: Information Retrieval\n",
      "<br>\n",
      "\n",
      "## Lecture 13: Language Models, Part I\n",
      "\n",
      "<br>\n",
      "\n",
      "### Dr. Aron Culotta\n",
      "### Illinois Institute of Technology\n",
      "### Spring 2014"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Last week...\n",
      "\n",
      "We ranked documents by: \n",
      "\n",
      "$P(R=1|d, q)$\n",
      "\n",
      "where $R=1$ means document $d$ is relevant to query $q$.\n",
      "\n",
      "**Problem:** hard to estimate this value without $d,q$ pairs labeled with relevance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Language Models\n",
      "\n",
      "**Idea:**\n",
      "\n",
      "Rank documents by:\n",
      "\n",
      "$P(q|d)$\n",
      "\n",
      "The probability that the process that generated $d$ would also generate $q$.\n",
      "\n",
      "No variable for relevance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Generative models\n",
      "- Each document is a list of strings from a language.\n",
      "- Consider all the possible documents the author could have written\n",
      "  - How many of them would contain the term \"zebra\"?\n",
      "- Consider the query $q$\n",
      "  - What is the probability that the author of document $d$ would have written down $q$?\n",
      "  - $P(q|M_d)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Finite State Machine\n",
      "\n",
      "Let a *language* $L$ be a set of documents $\\{d_1 \\ldots d_n\\}$.\n",
      "\n",
      "A finite-state machine $M_L$ accepts a document $d$ as input and outputs \"yes\" if $d \\in L$; otherwise it outputs \"no.\"\n",
      "\n",
      "$M_L$ consists of:\n",
      "\n",
      "- a set of **states** $S = \\{s_1 \\ldots s_m\\}$\n",
      "- an input **vocabulary** $V$, a finite set of acceptable terms\n",
      "- a **transition function** $\\delta : V \\times S \\mapsto S$ \n",
      "  - When in state $s_i$, if term $w \\in V$ is read, the state changes to $s_j$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=\"files/fsm.png\" width=\"50%\"/>\n",
      "\n",
      "\n",
      "- <font color=\"green\">Mr. John Smith Jr.</font> &nbsp;&nbsp; start $\\rightarrow$ prefix $\\xrightarrow{Mr.}$ first $\\xrightarrow{John}$ last $\\xrightarrow{Smith}$ suffix $\\xrightarrow{Jr.}$ accept\n",
      "- <font color=\"green\">Jane Doe</font>\n",
      "- <font color=\"red\">Mr. Jr.</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Weighted Finite State Machine\n",
      "\n",
      "<img src=\"files/wfsm.png\" width=\"50%\"/>\n",
      "\n",
      "\n",
      "- $P($<font color=\"green\">Mr. John Smith Jr.</font>$)= 0.4 * 1.0 * 1.0 * .05 * 1.0 = 0.02$ \n",
      "- $P($<font color=\"green\">Jane Doe</font>$) = 0.6 * 1.0 * 0.95 = 0.57$\n",
      "- $P($<font color=\"red\">Mr. Jr.</font>$) = 0.0$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Generative Model\n",
      "\n",
      "Rather than simply assigning probabilities to documents, we can use a weighted finite state machine to **generate** documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate names.\n",
      "# Assume all words are equally likely, but state transitions follow previous FSM.\n",
      "\n",
      "prefixes = ['Mr. ', 'Ms. ', 'Mrs. ', 'Dr. ']\n",
      "firsts = ['John ', 'Jane ', 'Jesse ']\n",
      "lasts = ['Smith ', 'Jones ', 'Doe ']\n",
      "suffixes = ['Jr. ', 'Sr. ', 'III ']\n",
      "\n",
      "def sample(alist):\n",
      "    \"\"\" Sample an element of a list. \"\"\"\n",
      "    return alist[random.randint(0, len(alist) - 1)]\n",
      "\n",
      "import random\n",
      "num_documents = 20\n",
      "for i in range(num_documents):\n",
      "    doc = ''\n",
      "    if random.random() <= 0.4:  # prefix\n",
      "        doc += sample(prefixes)\n",
      "    doc += sample(firsts) + sample(lasts)\n",
      "    if random.random() <= .05:  # suffix\n",
      "        doc += sample(suffixes)\n",
      "    print doc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "John Jones \n",
        "Jesse Doe \n",
        "Ms. Jane Smith \n",
        "Mrs. Jane Doe \n",
        "Jane Smith \n",
        "Jesse Doe \n",
        "John Smith \n",
        "Jane Jones \n",
        "John Doe \n",
        "Mrs. Jane Doe \n",
        "John Doe \n",
        "Mrs. John Smith \n",
        "Mr. Jane Doe \n",
        "Jesse Doe \n",
        "Jesse Smith \n",
        "Mrs. Jesse Doe \n",
        "Mr. John Doe \n",
        "Jane Doe \n",
        "Jane Doe \n",
        "Jane Jones \n"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Language Model\n",
      "\n",
      "A weighted finite state machine that can\n",
      "\n",
      "- generate documents\n",
      "- generate queries\n",
      "- assign probabilities to documents/queries\n",
      "\n",
      "\n",
      "**Idea:**\n",
      "\n",
      "- Construct a language model $M_d$ for each document $d$.\n",
      "- For each query $q$, compute the probability that $M_d$ generated $q$: $P(q|M_d)$\n",
      "- Rank documents by $P(q|M_d)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=\"files/lm2.png\" width=\"50%\"/>\n",
      "\n",
      "**How can we construct a language model from a document?**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Long history in natural language processing:\n",
      "\n",
      "- parse trees\n",
      "\n",
      "![parse](files/parse.jpg)\n",
      "\n",
      "Source: [Wikipedia](http://en.wikipedia.org/wiki/Parse_tree)\n",
      "\n",
      "- [sentence generators](http://writing-program.uchicago.edu/toys/randomsentence/)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "But, grammar has little effect on information retrieval.\n",
      "\n",
      "- queries are rarely grammatical"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Unigram Language Models\n",
      "\n",
      "- Ignore word order.\n",
      "- Generate each word independently."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "$\\begin{align} P(q|M_d) = \\prod_{t \\in q} P(t|M_d) = \\prod_{t \\in q} \\frac{tf_{t, d}}{L_d} \\end{align}$\n",
      "\n",
      "- $q:$ query consisting of terms $t$\n",
      "- $M_d:$ language model for document $d$\n",
      "- $tf_{t, d}:$ frequency of term $t$ in document $d$\n",
      "- $L_d:$ number of tokens in $d$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Unigram Language Models\n",
      "\n",
      "<img src=\"files/uni.png\" width=\"50%\"/>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "def doc2model(doc):\n",
      "    \"\"\" Convert a document d into a language model M_d. \"\"\"\n",
      "    counts = Counter(doc)\n",
      "    for term in counts:\n",
      "        counts[term] /= 1. * len(doc)\n",
      "    return counts\n",
      "\n",
      "m_d = doc2model(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'the', 'olympics'])\n",
      "print m_d"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counter({'the': 0.2, 'united': 0.1, 'gold': 0.1, 'states': 0.1, 'won': 0.1, 'nine': 0.1, 'in': 0.1, 'olympics': 0.1, 'medals': 0.1})\n"
       ]
      }
     ],
     "prompt_number": 120
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def sample_from_model(m_d, length):\n",
      "    \"\"\" Sample length words from language model m_d. \"\"\"\n",
      "    counts = np.random.multinomial(length, m_d.values(), size=1)[0]\n",
      "    words = []\n",
      "    for i, count in enumerate(counts):\n",
      "        words.extend(count * [m_d.keys()[i]])\n",
      "    return words\n",
      "    \n",
      "print sample_from_model(m_d, 10)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['united', 'gold', 'won', 'won', 'olympics', 'olympics', 'the', 'medals', 'medals', 'medals']\n"
       ]
      }
     ],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pr_q_given_m(q, m_d):\n",
      "    \"\"\" Compute P(q|M_d), the probability of language model M_d generating query q. \"\"\"\n",
      "    product = 1.\n",
      "    for qi in q:\n",
      "        product *= m_d[qi]\n",
      "    return product\n",
      "\n",
      "print 'Pr([the, olympics] | d)=', pr_q_given_m(['the', 'olympics'], m_d)\n",
      "print 'Pr([united, states] | d)=', pr_q_given_m(['united', 'states'], m_d)\n",
      "print 'Pr([olympics, united, states] | d)=', pr_q_given_m(['olympics', 'united', 'states'], m_d)\n",
      "    "
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pr([the, olympics] | d)= 0.02\n",
        "Pr([united, states] | d)= 0.01\n",
        "Pr([olympics, united, states] | d)= 0.001\n"
       ]
      }
     ],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def doc2ngram_model(doc, n):\n",
      "    \"\"\" Convert a document d into a language model M_d. \"\"\"\n",
      "    counts = Counter()\n",
      "    for i in range(len(doc) - 1):\n",
      "        counts.update([' '.join(doc[i:i+n]) for i in range(len(doc) - n + 1)])\n",
      "    length = sum(counts.values())\n",
      "    for term in counts:\n",
      "        counts[term] /= 1. * length\n",
      "    return counts\n",
      "\n",
      "m_d2 = doc2ngram_model(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'the', 'olympics'], 2)\n",
      "m_d3 = doc2ngram_model(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'the', 'slalom', 'in', 'the', 'olympics'], 2)\n",
      "\n",
      "print 'm_d2:', m_d2\n",
      "\n",
      "print '\\nm_d3', m_d3"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "m_d2: Counter({'states won': 0.1111111111111111, 'won nine': 0.1111111111111111, 'the olympics': 0.1111111111111111, 'united states': 0.1111111111111111, 'gold medals': 0.1111111111111111, 'the united': 0.1111111111111111, 'medals in': 0.1111111111111111, 'in the': 0.1111111111111111, 'nine gold': 0.1111111111111111})\n",
        "\n",
        "m_d3 Counter({'in the': 0.16666666666666666, 'states won': 0.08333333333333333, 'the slalom': 0.08333333333333333, 'won nine': 0.08333333333333333, 'the olympics': 0.08333333333333333, 'united states': 0.08333333333333333, 'gold medals': 0.08333333333333333, 'the united': 0.08333333333333333, 'medals in': 0.08333333333333333, 'nine gold': 0.08333333333333333, 'slalom in': 0.08333333333333333})\n"
       ]
      }
     ],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample_from_model(m_d3, 10)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 129,
       "text": [
        "['states won',\n",
        " 'the slalom',\n",
        " 'gold medals',\n",
        " 'the united',\n",
        " 'the united',\n",
        " 'medals in',\n",
        " 'in the',\n",
        " 'in the',\n",
        " 'in the',\n",
        " 'slalom in']"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m_d4 = doc2ngram_model(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'the', 'olympics'], 4)\n",
      "sample_from_model(m_d4, 10)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 130,
       "text": [
        "['nine gold medals in',\n",
        " 'nine gold medals in',\n",
        " 'gold medals in the',\n",
        " 'gold medals in the',\n",
        " 'won nine gold medals',\n",
        " 'won nine gold medals',\n",
        " 'the united states won',\n",
        " 'medals in the olympics',\n",
        " 'states won nine gold',\n",
        " 'states won nine gold']"
       ]
      }
     ],
     "prompt_number": 130
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Why not just set $n=10000$?**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 4-gram model:\n",
      "print 'Pr([the olympics] | m_d4)=', pr_q_given_m(['the olympics'], m_d4)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pr([the olympics] | m_d4)= 0.0\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 2-gram model\n",
      "print 'Pr([the olympics] | m_d2)=', pr_q_given_m(['the olympics'], m_d2)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pr([the olympics] | m_d2)= 0.111111111111\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Even for unigram model\n",
      "print 'Pr([the, olympics, zebra] | m_d)=', pr_q_given_m(['the', 'olympics', 'zebra'], m_d)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pr([the, olympics, zebra] | m_d)= 0.0\n"
       ]
      }
     ],
     "prompt_number": 112
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "If a query does not appear in document $d$, then $P(q|M_d)=0$.\n",
      "\n",
      "\n",
      "- Want to allow some chance that a word not in $d$ will appear."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def doc2model_smooth(doc, smooth_term, vocab):\n",
      "    \"\"\" Convert a document d into a language model M_d. \"\"\"\n",
      "    counts = Counter(doc)\n",
      "    for term in vocab:\n",
      "        counts[term] = (counts[term] + smooth_term) / (1. * len(doc) + smooth_term * len(vocab))\n",
      "    return counts\n",
      "\n",
      "vocab = ['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'olympics', 'zebra']\n",
      "m_d_smooth1 = doc2model_smooth(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'the', 'olympics'], 1, vocab)\n",
      "m_d_smooth10 = doc2model_smooth(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'the', 'olympics'], 10, vocab)\n",
      "print 'unsmoothed model:', m_d\n",
      "print '\\nsmoothed model1:', m_d_smooth1\n",
      "print '\\nsmoothed model10:', m_d_smooth10"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "unsmoothed model: Counter({'the': 0.2, 'united': 0.1, 'gold': 0.1, 'states': 0.1, 'won': 0.1, 'nine': 0.1, 'in': 0.1, 'olympics': 0.1, 'medals': 0.1})\n",
        "\n",
        "smoothed model1: Counter({'the': 0.15, 'united': 0.1, 'gold': 0.1, 'states': 0.1, 'won': 0.1, 'nine': 0.1, 'in': 0.1, 'olympics': 0.1, 'medals': 0.1, 'zebra': 0.05})\n",
        "\n",
        "smoothed model10: Counter({'the': 0.10909090909090909, 'united': 0.1, 'gold': 0.1, 'states': 0.1, 'won': 0.1, 'nine': 0.1, 'in': 0.1, 'olympics': 0.1, 'medals': 0.1, 'zebra': 0.09090909090909091})\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Pr([the, olympics, zebra] | m_d_smooth1)=', pr_q_given_m(['the', 'olympics', 'zebra'], m_d_smooth1)\n",
      "print 'Pr([the, olympics, zebra] | m_d_smooth10)=', pr_q_given_m(['the', 'olympics', 'zebra'], m_d_smooth10)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pr([the, olympics, zebra] | m_d_smooth1)= 0.00075\n",
        "Pr([the, olympics, zebra] | m_d_smooth10)= 0.00099173553719\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Smoothed Language Model\n",
      "\n",
      "(Laplace smoothing)\n",
      "\n",
      "$\\begin{align} P_{smooth}(q|M_d) = \\prod_{t \\in q} P(t|M_d) = \\prod_{t \\in q} \\frac{tf_{t, d} + \\epsilon}{L_d + V\\epsilon} \\end{align}$\n",
      "\n",
      "- $q:$ query consisting of terms $t$\n",
      "- $M_d:$ language model for document $d$\n",
      "- $tf_{t, d}:$ frequency of term $t$ in document $d$\n",
      "- $L_d:$ number of tokens in $d$\n",
      "- $\\epsilon:$ amount to smooth\n",
      "- $V$: vocabulary size"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}