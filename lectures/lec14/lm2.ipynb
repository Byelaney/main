{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# CS 429: Information Retrieval\n",
      "<br>\n",
      "\n",
      "## Lecture 14: Language Models, Part II\n",
      "\n",
      "<br>\n",
      "\n",
      "### Dr. Aron Culotta\n",
      "### Illinois Institute of Technology\n",
      "### Spring 2014"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Language Models\n",
      "\n",
      "**Idea:**\n",
      "\n",
      "Rank documents by:\n",
      "\n",
      "$P(q|d)$\n",
      "\n",
      "The probability that the process that generated $d$ would also generate $q$.\n",
      "\n",
      "No variable for relevance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Generative models\n",
      "- Each document is a list of strings from a language.\n",
      "- Consider all the possible documents the author could have written\n",
      "  - How many of them would contain the term \"zebra\"?\n",
      "- Consider the query $q$\n",
      "  - What is the probability that the author of document $d$ would have written down $q$?\n",
      "  - $P(q|M_d)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Finite State Machine\n",
      "\n",
      "Let a *language* $L$ be a set of documents $\\{d_1 \\ldots d_n\\}$.\n",
      "\n",
      "A finite-state machine $M_L$ accepts a document $d$ as input and outputs \"yes\" if $d \\in L$; otherwise it outputs \"no.\"\n",
      "\n",
      "$M_L$ consists of:\n",
      "\n",
      "- a set of **states** $S = \\{s_1 \\ldots s_m\\}$\n",
      "- an input **vocabulary** $V$, a finite set of acceptable terms\n",
      "- a **transition function** $\\delta : V \\times S \\mapsto S$ \n",
      "  - When in state $s_i$, if term $w \\in V$ is read, the state changes to $s_j$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=\"files/fsm.png\" width=\"50%\"/>\n",
      "\n",
      "\n",
      "- <font color=\"green\">Mr. John Smith Jr.</font> &nbsp;&nbsp; start $\\rightarrow$ prefix $\\xrightarrow{Mr.}$ first $\\xrightarrow{John}$ last $\\xrightarrow{Smith}$ suffix $\\xrightarrow{Jr.}$ accept\n",
      "- <font color=\"green\">Jane Doe</font>\n",
      "- <font color=\"red\">Mr. Jr.</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Weighted Finite State Machine\n",
      "\n",
      "<img src=\"files/wfsm.png\" width=\"50%\"/>\n",
      "\n",
      "\n",
      "- $P($<font color=\"green\">Mr. John Smith Jr.</font>$)= 0.4 * 1.0 * 1.0 * .05 * 1.0 = 0.02$ \n",
      "- $P($<font color=\"green\">Jane Doe</font>$) = 0.6 * 1.0 * 0.95 = 0.57$\n",
      "- $P($<font color=\"red\">Mr. Jr.</font>$) = 0.0$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Generative Model\n",
      "\n",
      "Rather than simply assigning probabilities to documents, we can use a weighted finite state machine to **generate** documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate names.\n",
      "# Assume all words are equally likely, but state transitions follow previous FSM.\n",
      "\n",
      "prefixes = ['Mr. ', 'Ms. ', 'Mrs. ', 'Dr. ']\n",
      "firsts = ['John ', 'Jane ', 'Jesse ']\n",
      "lasts = ['Smith ', 'Jones ', 'Doe ']\n",
      "suffixes = ['Jr. ', 'Sr. ', 'III ']\n",
      "\n",
      "def sample(alist):\n",
      "    \"\"\" Sample an element of a list. \"\"\"\n",
      "    return alist[random.randint(0, len(alist) - 1)]\n",
      "\n",
      "import random\n",
      "num_documents = 20\n",
      "for i in range(num_documents):\n",
      "    doc = ''\n",
      "    if random.random() <= 0.4:  # prefix\n",
      "        doc += sample(prefixes)\n",
      "    doc += sample(firsts) + sample(lasts)\n",
      "    if random.random() <= .05:  # suffix\n",
      "        doc += sample(suffixes)\n",
      "    print doc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jesse Doe \n",
        "Mrs. Jesse Jones \n",
        "Dr. Jane Doe \n",
        "Jesse Jones \n",
        "Ms. Jane Jones \n",
        "Jane Doe \n",
        "Mr. Jesse Jones \n",
        "Jesse Doe \n",
        "Mr. John Smith \n",
        "Jesse Jones \n",
        "Jesse Smith \n",
        "Dr. Jesse Jones \n",
        "Jesse Smith \n",
        "Jane Smith \n",
        "Ms. Jesse Smith \n",
        "Mr. John Doe \n",
        "Dr. John Doe \n",
        "Mrs. John Smith \n",
        "Dr. Jane Doe Sr. \n",
        "Jesse Jones \n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Language Model\n",
      "\n",
      "A weighted finite state machine that can\n",
      "\n",
      "- generate documents\n",
      "- generate queries\n",
      "- assign probabilities to documents/queries\n",
      "\n",
      "\n",
      "**Idea:**\n",
      "\n",
      "- Construct a language model $M_d$ for each document $d$.\n",
      "- For each query $q$, compute the probability that $M_d$ generated $q$: $P(q|M_d)$\n",
      "- Rank documents by $P(q|M_d)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=\"files/lm2.png\" width=\"50%\"/>\n",
      "\n",
      "**How can we construct a language model from a document?**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Long history in natural language processing:\n",
      "\n",
      "- parse trees\n",
      "\n",
      "![parse](files/parse.jpg)\n",
      "\n",
      "Source: [Wikipedia](http://en.wikipedia.org/wiki/Parse_tree)\n",
      "\n",
      "- [sentence generators](http://writing-program.uchicago.edu/toys/randomsentence/)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "But, grammar has little effect on information retrieval.\n",
      "\n",
      "- queries are rarely grammatical"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Unigram Language Models\n",
      "\n",
      "- Ignore word order.\n",
      "- Generate each word independently."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "$\\begin{align} P(q|M_d) = \\prod_{t \\in q} P(t|M_d) = \\prod_{t \\in q} \\frac{tf_{t, d}}{L_d} \\end{align}$\n",
      "\n",
      "- $q:$ query consisting of terms $t$\n",
      "- $M_d:$ language model for document $d$\n",
      "- $tf_{t, d}:$ frequency of term $t$ in document $d$\n",
      "- $L_d:$ number of tokens in $d$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Unigram Language Models\n",
      "\n",
      "<img src=\"files/uni.png\" width=\"50%\"/>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "def doc2model(doc):\n",
      "    \"\"\" Convert a document d into a language model M_d. \"\"\"\n",
      "    counts = Counter(doc)\n",
      "    for term in counts:\n",
      "        counts[term] /= 1. * len(doc)\n",
      "    return counts\n",
      "\n",
      "m_d = doc2model(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'the', 'olympics'])\n",
      "print m_d"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counter({'the': 0.2, 'united': 0.1, 'gold': 0.1, 'states': 0.1, 'won': 0.1, 'nine': 0.1, 'in': 0.1, 'olympics': 0.1, 'medals': 0.1})\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def sample_from_model(m_d, length):\n",
      "    \"\"\" Sample length words from language model m_d. \"\"\"\n",
      "    counts = np.random.multinomial(length, m_d.values(), size=1)[0]\n",
      "    words = []\n",
      "    for i, count in enumerate(counts):\n",
      "        words.extend(count * [m_d.keys()[i]])\n",
      "    return words\n",
      "    \n",
      "print sample_from_model(m_d, 10)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['united', 'united', 'united', 'united', 'states', 'won', 'nine', 'nine', 'the', 'medals']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pr_q_given_m(q, m_d):\n",
      "    \"\"\" Compute P(q|M_d), the probability of language model M_d generating query q. \"\"\"\n",
      "    product = 1.\n",
      "    for qi in q:\n",
      "        product *= m_d[qi]\n",
      "    return product\n",
      "\n",
      "print 'Pr([the, olympics] | d)=', pr_q_given_m(['the', 'olympics'], m_d)\n",
      "print 'Pr([united, states] | d)=', pr_q_given_m(['united', 'states'], m_d)\n",
      "print 'Pr([olympics, united, states] | d)=', pr_q_given_m(['olympics', 'united', 'states'], m_d)\n",
      "    "
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pr([the, olympics] | d)= 0.02\n",
        "Pr([united, states] | d)= 0.01\n",
        "Pr([olympics, united, states] | d)= 0.001\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def doc2ngram_model(doc, n):\n",
      "    \"\"\" Convert a document d into a language model M_d. \"\"\"\n",
      "    counts = Counter()\n",
      "    for i in range(len(doc) - 1):\n",
      "        counts.update([' '.join(doc[i:i+n]) for i in range(len(doc) - n + 1)])\n",
      "    length = sum(counts.values())\n",
      "    for term in counts:\n",
      "        counts[term] /= 1. * length\n",
      "    return counts\n",
      "\n",
      "m_d2 = doc2ngram_model(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'the', 'olympics'], 2)\n",
      "m_d3 = doc2ngram_model(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'the', 'slalom', 'in', 'the', 'olympics'], 2)\n",
      "\n",
      "print 'm_d2:', m_d2\n",
      "\n",
      "print '\\nm_d3', m_d3"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "m_d2: Counter({'states won': 0.1111111111111111, 'won nine': 0.1111111111111111, 'the olympics': 0.1111111111111111, 'united states': 0.1111111111111111, 'gold medals': 0.1111111111111111, 'the united': 0.1111111111111111, 'medals in': 0.1111111111111111, 'in the': 0.1111111111111111, 'nine gold': 0.1111111111111111})\n",
        "\n",
        "m_d3 Counter({'in the': 0.16666666666666666, 'states won': 0.08333333333333333, 'the slalom': 0.08333333333333333, 'won nine': 0.08333333333333333, 'the olympics': 0.08333333333333333, 'united states': 0.08333333333333333, 'gold medals': 0.08333333333333333, 'the united': 0.08333333333333333, 'medals in': 0.08333333333333333, 'nine gold': 0.08333333333333333, 'slalom in': 0.08333333333333333})\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample_from_model(m_d3, 10)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "['states won',\n",
        " 'states won',\n",
        " 'the slalom',\n",
        " 'the slalom',\n",
        " 'won nine',\n",
        " 'the olympics',\n",
        " 'the olympics',\n",
        " 'the united',\n",
        " 'medals in',\n",
        " 'in the']"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m_d4 = doc2ngram_model(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'the', 'olympics'], 4)\n",
      "sample_from_model(m_d4, 10)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "['nine gold medals in',\n",
        " 'nine gold medals in',\n",
        " 'won nine gold medals',\n",
        " 'the united states won',\n",
        " 'medals in the olympics',\n",
        " 'states won nine gold',\n",
        " 'states won nine gold',\n",
        " 'states won nine gold',\n",
        " 'united states won nine',\n",
        " 'united states won nine']"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Why not just set $n=10000$?**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 4-gram model:\n",
      "print 'Pr([the olympics] | m_d4)=', pr_q_given_m(['the olympics'], m_d4)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pr([the olympics] | m_d4)= 0.0\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 2-gram model\n",
      "print 'Pr([the olympics] | m_d2)=', pr_q_given_m(['the olympics'], m_d2)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pr([the olympics] | m_d2)= 0.111111111111\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Even for unigram model\n",
      "print 'Pr([the, olympics, zebra] | m_d)=', pr_q_given_m(['the', 'olympics', 'zebra'], m_d)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pr([the, olympics, zebra] | m_d)= 0.0\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "If a query does not appear in document $d$, then $P(q|M_d)=0$.\n",
      "\n",
      "\n",
      "- Want to allow some chance that a word not in $d$ will appear."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Smoothed Language Model\n",
      "\n",
      "(Laplace smoothing)\n",
      "\n",
      "$\\begin{align} P_{smooth}(q|M_d) = \\prod_{t \\in q} P(t|M_d) = \\prod_{t \\in q} \\frac{tf_{t, d} + \\epsilon}{L_d + V\\epsilon} \\end{align}$\n",
      "\n",
      "- $q:$ query consisting of terms $t$\n",
      "- $M_d:$ language model for document $d$\n",
      "- $tf_{t, d}:$ frequency of term $t$ in document $d$\n",
      "- $L_d:$ number of tokens in $d$\n",
      "- $\\epsilon:$ amount to smooth\n",
      "- $V$: vocabulary size"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def doc2model_smooth(doc, smooth_term, vocab):\n",
      "    \"\"\" Convert a document d into a language model M_d. \"\"\"\n",
      "    counts = Counter(doc)\n",
      "    for term in vocab:\n",
      "        counts[term] = (counts[term] + smooth_term) / (1. * len(doc) + smooth_term * len(vocab))\n",
      "    return counts\n",
      "\n",
      "vocab = ['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'olympics', 'zebra', 'a']\n",
      "m_d_smooth1 = doc2model_smooth(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'the', 'olympics'], 1, vocab)\n",
      "m_d_smooth10 = doc2model_smooth(['the', 'united', 'states', 'won', 'nine', 'gold', 'medals', 'in', 'the', 'olympics'], 10, vocab)\n",
      "print 'unsmoothed model:', m_d\n",
      "print '\\nsmoothed model1:', m_d_smooth1\n",
      "print '\\nsmoothed model10:', m_d_smooth10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "unsmoothed model: Counter({'the': 0.2, 'united': 0.1, 'gold': 0.1, 'states': 0.1, 'won': 0.1, 'nine': 0.1, 'in': 0.1, 'olympics': 0.1, 'medals': 0.1})\n",
        "\n",
        "smoothed model1: Counter({'the': 0.14285714285714285, 'united': 0.09523809523809523, 'gold': 0.09523809523809523, 'states': 0.09523809523809523, 'won': 0.09523809523809523, 'nine': 0.09523809523809523, 'in': 0.09523809523809523, 'olympics': 0.09523809523809523, 'medals': 0.09523809523809523, 'a': 0.047619047619047616, 'zebra': 0.047619047619047616})\n",
        "\n",
        "smoothed model10: Counter({'the': 0.1, 'united': 0.09166666666666666, 'gold': 0.09166666666666666, 'states': 0.09166666666666666, 'won': 0.09166666666666666, 'nine': 0.09166666666666666, 'in': 0.09166666666666666, 'olympics': 0.09166666666666666, 'medals': 0.09166666666666666, 'a': 0.08333333333333333, 'zebra': 0.08333333333333333})\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Pr([the, olympics, zebra] | m_d_smooth1)=', pr_q_given_m(['the', 'olympics', 'zebra'], m_d_smooth1)\n",
      "print 'Pr([the, olympics, zebra] | m_d_smooth10)=', pr_q_given_m(['the', 'olympics', 'zebra'], m_d_smooth10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pr([the, olympics, zebra] | m_d_smooth1)= 0.000647878198899\n",
        "Pr([the, olympics, zebra] | m_d_smooth10)= 0.000763888888889\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "**Problem with Laplace smoothing:**\n",
      "\n",
      "- Assumes that all unseen words are equally likely.\n",
      "  - Effectively adds $\\epsilon$ occurrences to every document. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Pr([the, olympics, zebra] | m_d_smooth10)=', pr_q_given_m(['the', 'olympics', 'zebra'], m_d_smooth10)\n",
      "print 'Pr([the, olympics, a] | m_d_smooth10)=', pr_q_given_m(['the', 'olympics', 'a'], m_d_smooth10)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pr([the, olympics, zebra] | m_d_smooth10)= 0.000763888888889\n",
        "Pr([the, olympics, a] | m_d_smooth10)= 0.000763888888889\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "- $d_1:$ the, cat\n",
      "- $d_2:$ dog, cat\n",
      "\n",
      "\n",
      "- $q:$ dog, the\n",
      "\n",
      "Should return $d_2$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "But, Laplace smoothing means missing the word \"dog\" is just as bad as missing the word \"the\".\n",
      "\n",
      "\n",
      "$\\begin{align} P_{smooth}(q|M_d) = \\prod_{t \\in q} P(t|M_d) = \\prod_{t \\in q} \\frac{tf_{t, d} + \\epsilon}{L_d + V\\epsilon} \\end{align}$\n",
      "\n",
      "$\\begin{align} P_{smooth}(q|M_{d_1}) = P(dog|M_{d_1}) * P(the|M_{d_1}) = \\frac{\\epsilon}{2 + V\\epsilon} * \\frac{1 + \\epsilon}{2 + V\\epsilon} \\end{align}$\n",
      "\n",
      "$\\begin{align} P_{smooth}(q|M_{d_2}) = P(dog|M_{d_2}) * P(the|M_{d_2}) = \\frac{1 + \\epsilon}{2 + V\\epsilon} * \\frac{\\epsilon}{2 + V\\epsilon} \\end{align}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Smoothing with collection frequency\n",
      "\n",
      "Let $cf_t$ be the collection frequency of term $t$\n",
      "\n",
      "- That is, the total number of times it occurs (as opposed to $df_t$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Then if term $t$ does not appear in document $d$. \n",
      "\n",
      "- We want $P(t|M_d) < \\frac{cf_t}{T}$\n",
      "- $T=$ total number of tokens in all documents."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Let $M_c$ be the language model for the entire document collection:\n",
      "\n",
      "\n",
      "$\\begin{align} P(t|M_c) = \\frac{cf_{t}}{T} \\end{align}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Dirichlet Smoothing\n",
      "\n",
      "\n",
      "$\\begin{align} P_{dir}(t|M_d) = \\frac{tf_{t, d} + \\alpha P(t|M_c)}{L_d + \\alpha} \\end{align}$\n",
      "\n",
      "- $\\alpha:$ tunable parameter\n",
      "- Larger $\\alpha \\rightarrow$ more smoothing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "# Interpolation Smoothing\n",
      "\n",
      "Alternatively, we can *interpolate* between the document probability and the collection probability:\n",
      "\n",
      "$\\begin{align}P_{interp}(t|M_d) & = & \\lambda P(t|M_d) + (1-\\lambda) P(t|M_c)\\\\\n",
      "& = & \\lambda \\frac{tf_{t, d}}{L_d} + (1-\\lambda) \\frac{cf_{t}}{T} \n",
      "\\end{align}$\n",
      "\n",
      "- $\\lambda$ is a tunable parameter.\n",
      "- Smaller $\\lambda \\rightarrow$ more smoothing.\n",
      "- This is also called *Jelinek-Mercer* smoothing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Thus, the new query likelihood is:\n",
      "\n",
      "\n",
      "$\\begin{align} P_{interp}(q|M_d) = \\prod_{t \\in q} P_{interp}(t|M_d) = \\prod_{t \\in q} \\lambda \\frac{tf_{t, d}}{L_d} + (1-\\lambda) \\frac{cf_{t}}{T}  \\end{align}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Interpolation Example\n",
      "\n",
      "(from [MRS](http://nlp.stanford.edu/IR-book/pdf/12lmodel.pdf) p. 246)\n",
      "\n",
      "- $d_1:$ Xyzzy reports a pro\ufb01t but revenue is down\n",
      "- $d_2:$ Quorus narrows quarter loss but revenue decreases further\n",
      "- $\\lambda=.5$\n",
      "\n",
      "Suppose the query is **revenue down**. Then:\n",
      "\n",
      "$P_{interp}(q|d_1) = $\n",
      "\n",
      "$P_{interp}(q|d_2) = $\n",
      "\n",
      "$\\begin{align}P_{interp}(t|M_d) & = & \\lambda P(t|M_d) + (1-\\lambda) P(t|M_c)\\\\\n",
      "& = & \\lambda \\frac{tf_{t, d}}{L_d} + (1-\\lambda) \\frac{cf_{t}}{T} \n",
      "\\end{align}$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Interpolation Example\n",
      "\n",
      "(from [MRS](http://nlp.stanford.edu/IR-book/pdf/12lmodel.pdf) p. 246)\n",
      "\n",
      "- $d_1:$ Xyzzy reports a pro\ufb01t but revenue is down\n",
      "- $d_2:$ Quorus narrows quarter loss but revenue decreases further\n",
      "- $\\lambda=.5$\n",
      "\n",
      "Suppose the query is revenue down. Then:\n",
      "\n",
      "$P_{interp}(q|d_1) = [(1/8 + 2/16)/2] * [(1/8 + 1/16)/2] = 1/8 * 3/32 = 3/256$\n",
      "\n",
      "$P_{interp}(q|d_2) = [(1/8 + 2/16)/2] * [(0/8 + 1/16)/2] = 1/8 * 1/32 = 1/256$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Where are the following quantities used, if at all?\n",
      "\n",
      "\n",
      "- Term frequency in a document\n",
      "- Collection frequency of a term\n",
      "- Document frequency of a term\n",
      "- Length normalization of a term\n",
      "\n",
      "\n",
      "$\\begin{align} P_{interp}(q|M_d) = \\prod_{t \\in q} P_{interp}(t|M_d) = \\prod_{t \\in q} \\lambda \\frac{tf_{t, d}}{L_d} + (1-\\lambda) \\frac{cf_{t}}{T}  \\end{align}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "**Should amount of smoothing ($\\lambda)$ depend on query length?**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "![qlength](files/qlength.png)\n",
      "\n",
      "Source: [Zhai & Lafferty, 2004](http://galton.uchicago.edu/~lafferty/pdf/smooth-tois.pdf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Language Model vs. tf-idf\n",
      "\n",
      "![lmvstfidf](files/lmvstfidf.png)\n",
      "\n",
      "Source: [MRS](http://nlp.stanford.edu/IR-book/pdf/12lmodel.pdf)\n",
      "\n",
      "\n",
      "\n",
      "$\\begin{align} P_{interp}(q|M_d) = \\prod_{t \\in q} P_{interp}(t|M_d) = \\prod_{t \\in q} \\lambda \\frac{tf_{t, d}}{L_d} + (1-\\lambda) \\frac{cf_{t}}{T}  \\end{align}$\n",
      "\n",
      "vs.\n",
      "\n",
      "$ S_{tfidf}(q, d) = \\sum_{t \\in q} \\begin{align} (1 + \\log(tf_{t, d})) * \\log(\\frac{N}{df_t}) \\end{align}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<table>\n",
      "<tr><td>docID</td> <td>Document text</td></tr>\n",
      "<tr> <td>1</td> <td>click go the shears boys click click click</td> </tr>\n",
      "<tr> <td>2</td> <td>click click</td></tr>\n",
      "<tr> <td>3</td> <td>metal here</td></tr>\n",
      "<tr> <td>4</td> <td>metal shears click here</td></tr>\n",
      "</table>\n",
      "\n",
      "<table>\n",
      "<tr><td>Query</td> <td>Doc 1</td> <td>Doc 2</td> <td>Doc 3</td> <td>Doc 4</td></tr>\n",
      "<tr><td>click</td><td> </td><td> </td><td> </td><td> </td></tr>\n",
      "<tr><td>shears</td><td> </td><td> </td><td> </td><td> </td></tr>\n",
      "<tr><td>click shears</td><td> </td><td> </td><td> </td><td> </td></tr>\n",
      "</table>\n",
      "\n",
      "Let $\\lambda=0.5$.\n",
      "\n",
      "$\\begin{align} P_{interp}(q|M_d) = \\prod_{t \\in q} P_{interp}(t|M_d) = \\prod_{t \\in q} \\lambda \\frac{tf_{t, d}}{L_d} + (1-\\lambda) \\frac{cf_{t}}{T}  \\end{align}$\n",
      "\n",
      "(Source: [MRS](http://nlp.stanford.edu/IR-book/pdf/12lmodel.pdf))"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}